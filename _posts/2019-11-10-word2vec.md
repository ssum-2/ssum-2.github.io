---
title: "Word2vec"
classes: wide
categories:
  - Study
tags:
  - AI
  - ML
author_profile : true
use_math : true
last_modified_at: 2019-11-10
---

> 'ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹2' ì˜ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì‘ì„±í•˜ì˜€ìŒ.



###  ğŸ¤ NLPì˜ Embedding

- ì‚¬ëŒì˜ ì–¸ì–´ë¥¼ ê¸°ê³„ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ì ë‚˜ì—´ì¸ "Vector"ë¡œ ë°”ê¾¼ ê²°ê³¼ ë˜ëŠ” ê³¼ì •
- ë‹¨ì–´ë‚˜ ë¬¸ì¥ì„ Vectorë¡œ ë³€í™˜í•˜ì—¬ Vector Spaceë¡œ ë¼ì›Œë„£ëŠ”ë‹¤("Embed") => Embedding



### ğŸ™ŠWord2vec?

- 2013ë…„ Googleì´ ë°œí‘œí•œ ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ë¡œ, **ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜** í•˜ëŠ” ê¸°ë²•

  > -  Efficient Estimation of Word Representations in Vector Space
  >
  >   â€‹	https://arxiv.org/pdf/1301.3781.pdf
  >
  >   - Skip-gramê³¼ CBOWë¼ëŠ” ë‘ ê°€ì§€ ë°©ë²•ë¡ ì„ ì œì‹œ
  >   - Skip-gram; ì¤‘ì‹¬ì— ìˆëŠ” ë‹¨ì–´(target)ë¡œ ì£¼ë³€ ë‹¨ì–´(context)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•
  >   - CBOW; ì£¼ë³€ì— ìˆëŠ” ë‹¨ì–´(context)ë“¤ì„ ê°€ì§€ê³  ì¤‘ì‹¬ì— ìˆëŠ” ë‹¨ì–´(target)ë¥¼ ë§ì¶”ëŠ” ë°©ë²•
  >
  > - Distributed Representations of Words and Phrases and their Compositionality
  >
  >   https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  >
  >   - Skip-gram, CBOW ë‘ ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ë“±ì˜ í•™ìŠµ ìµœì í™” ê¸°ë²•ì„ ì œì•ˆ

- ë§ë­‰ì¹˜(corpus) -> í˜•íƒœì†Œë¶„ì„ -> 100ì°¨ì›ìœ¼ë¡œ í•™ìŠµ -> ë‹¨ì–´ë²¡í„° ì‚¬ì´ì˜ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥

- ë²¡í„° ê³µê°„ì„ ì‹œê°í™” í•  ìˆ˜ ìˆìŒ (ex. t-SNE)

  ![source: imgur.com](http://i.imgur.com/83gI8Gl.jpg)

- ë‹¨ì–´ë²¡í„° ê°„ ì‚¬ì¹™ì—°ì‚°ìœ¼ë¡œ ë‹¨ì–´ ìœ ì¶” í‰ê°€ ê°€ëŠ¥

- Word2Vecì˜ ì„ë² ë”©ì„ ë‹¤ë¥¸ ë”¥ëŸ¬ë‹ì˜ ì…ë ¥ê°’(Input)ìœ¼ë¡œ ì‚¬ìš©; Transfer Learning(ì „ì´ í•™ìŠµ)



### 3.1 ì¶”ë¡  ê¸°ë°˜ ê¸°ë²•ê³¼ ì‹ ê²½ë§

- ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì€ ë¶„í¬ ê°€ì„¤ì„ ë°°ê²½ìœ¼ë¡œí•œ **í†µê³„ ê¸°ë°˜ ê¸°ë²•** ê³¼ **ì¶”ë¡  ê¸°ë°˜ ê¸°ë²•** ë‘ ë¶€ë¥˜ë¡œ ë‚˜ë‰¨
  - ë¶„í¬ ê°€ì„¤; "ë‹¨ì–´ì˜ ì˜ë¯¸ëŠ” ì£¼ë³€ ë‹¨ì–´ì— ì˜í•´ í˜•ì„±ëœë‹¤"ëŠ” ê°€ì„¤ --> ë‹¨ì–´ì˜ ë™ì‹œë°œìƒ ê°€ëŠ¥ì„±ì„ ì–¼ë§ˆë‚˜ ì˜ ëª¨ë¸ë§ í•˜ëŠ”ê°€ì— ì¤‘ìš”í•œ ì—°êµ¬ í¬ì¸íŠ¸
- í†µê³„ ê¸°ë°˜ ê¸°ë²•ì˜ ë¬¸ì œì  -> ê·¹ë³µ ëŒ€ì•ˆìœ¼ë¡œì„œì˜ ì¶”ë¡  ê¸°ë°˜ ê¸°ë²•, word2vec ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹ ê²½ë§ìœ¼ë¡œ ë‹¨ì–´ ì²˜ë¦¬í•˜ëŠ” ì˜ˆë¥¼ ì œì‹œ

##### 3.1.1. í†µê³„ ê¸°ë°˜ ê¸°ë²•ì˜ ë¬¸ì œì 

- í†µê³„ ê¸°ë°˜ ê¸°ë²•
  - ì£¼ë³€ ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ê¸°ì´ˆë¡œ ë‹¨ì–´ë¥¼ í‘œí˜„
  - ë§ë­‰ì¹˜ì˜ ì „ì²´ í†µê³„(ë™ì‹œë°œìƒ í–‰ë ¬, PPMI ë“±) -> SVD(íŠ¹ì´ê°’ ë¶„í•´)ë¥¼ ì ìš©(1íšŒ) -> ë°€ì§‘ë²¡í„°(Dense vector; ë‹¨ì–´ì˜ ë¶„ì‚°í‘œí˜„)
  - => BUT! ëŒ€ê·œëª¨ ë§ë­‰ì¹˜ë¥¼ ë‹¤ë£° ë•Œ ë¬¸ì œ ë°œìƒ
- ì¶”ë¡  ê¸°ë°˜ ê¸°ë²•
  - ì‹ ê²½ë§ì—ì„œì˜ ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ
    - ì‹ ê²½ë§ì´ í•œë²ˆì— ë¯¸ë‹ˆë°°ì¹˜ì˜ í•™ìŠµ ìƒ˜í”Œì”© ë°˜ë³µí•´ì„œ í•™ìŠµí•˜ë©° ê°€ì¤‘ì¹˜ë¥¼ ê°±ì‹ 
- <img src="https://imgur.com/ItHBS5I.jpg" alt="https://imgur.com/ItHBS5I.jpg" style="zoom:33%;" /> <img src="https://imgur.com/1JBytm3.jpg" alt="https://imgur.com/1JBytm3.jpg" style="zoom:33%;" /> 
  - í†µê³„ ê¸°ë°˜ ê¸°ë²•ì€ ë°°ì¹˜ í•™ìŠµìœ¼ë¡œ ë°ì´í„°ë¥¼ í•œë²ˆì— ì²˜ë¦¬
  - ì¶”ë¡  ê¸°ë°˜ ê¸°ë²•ì€ ë¯¸ë‹ˆ ë°°ì¹˜ í•™ìŠµìœ¼ë¡œ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ
    - ë³‘ë ¬ ê³„ì‚° ë“±ì´ ê°€ëŠ¥í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŒ

##### 3.1.2 ì¶”ë¡  ê¸°ë°˜ ê¸°ë²• ê°œìš”

>  you ____ goodbye and I say hello.

- ì£¼ë³€ ë‹¨ì–´(Context word)ê°€ ìˆì„ ë•Œ "ã…¡ã…¡"ë¡œ í‘œí˜„ëœ ì¤‘ì‹¬ ë‹¨ì–´(Target word)ê°€ ë¬´ì—‡ì¸ì§€ **ì¶”ë¡ **
- ì¶”ë¡  ë¬¸ì œë¥¼ ë°˜ë³µí•˜ì—¬ í’€ë©´ì„œ ë‹¨ì–´ì˜ ì¶œí˜„ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê²ƒ
- <img src="https://imgur.com/SINTF67.png" alt="https://imgur.com/SINTF67.png" style="zoom: 33%;" align="left" />
  - ë§¥ë½ ì •ë³´ë¥¼ ì…ë ¥ ë°›ì•„ ê° ë‹¨ì–´ì˜ ì¶œí˜„ í™•ë¥ ì„ ì¶œë ¥ -> ë§ë­‰ì¹˜ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ ì¶”ì¸¡ì„ ë‚´ë†“ë„ë¡ í•™ìŠµ -> í•™ìŠµì˜ ê²°ê³¼ë¡œ ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ì„ ì–»ìŒ



##### 3.1.3. ì‹ ê²½ë§ì—ì„œì˜ ë‹¨ì–´ ì²˜ë¦¬

- ë‹¨ì–´ë¥¼ ê³ ì • ê¸¸ì´ì˜ ë²¡í„°ë¡œ ë³€í™˜ --> **one-hot encoding** ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ì›-í•« ë²¡í„°ë¡œ ë³€í™˜
  - ì›-í•« ì¸ì½”ë”©?; ì •ìˆ˜ ì¸ì½”ë”©ì„í†µí•´ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬í•œ í›„ ìˆ«ìë¡œ ë°”ë€ ë‹¨ì–´ë“¤ì„ ë²¡í„°í™”í•œ ì¼ì¢…ì˜ í¬ì†Œ í‘œí˜„(Sparse Representation)
  - ê°€ì§€ê³  ìˆëŠ” í…ìŠ¤íŠ¸ì— ë‹¨ì–´ì˜ ê°œìˆ˜ == ë‹¨ì–´ì§‘í•©ì˜ í¬ê¸° â†’ ë‹¨ì–´ì˜ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚  ìˆ˜ë¡ ë¹„íš¨ìœ¨ì 
  - ë‹¨ì–´ì§‘í•©ì˜ í¬ê¸°ë¥¼ ë²¡í„°ì˜ ì°¨ì›ìœ¼ë¡œ ë³´ê³ , í‘œí˜„í•˜ë ¤ëŠ” Trueì˜ ì¸ë±ìŠ¤ì— 1, ì•„ë‹Œ ê²½ìš°ì—ëŠ” 0ì„ ë¶€ì—¬. (0ê³¼ 1ë¡œë§Œ í‘œí˜„ë¨)
  - ì—°ì‚°ì˜ í¸ì˜ì„±, ë‹¤ì–‘í•œ ëª¨ë¸ ì ìš©, ì •í™•ë„ í–¥ìƒ ë“±ì˜ ì´ìœ ë¡œ ì‚¬ìš©
  - <img src="https://imgur.com/n3vXi7M.png" alt="https://imgur.com/n3vXi7M.png" style="zoom: 50%;" align="left"/>
- ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë‚˜íƒ€ë‚´ê³  ì‹ ê²½ë§ì„ êµ¬ì„±í•˜ëŠ” ê³„ì¸µë“¤ì€ ë²¡í„°ë¥¼ ì²˜ë¦¬ --> ë‹¨ì–´ë¥¼ ì‹ ê²½ë§ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ
  <img src="https://imgur.com/AmTrU0S.png" alt="https://imgur.com/AmTrU0S.png" style="zoom:33%;" align="left"/><img src="https://imgur.com/FuTGgCX.png" alt="https://imgur.com/FuTGgCX.png" style="zoom:33%;" />
  - ì™„ì „ì—°ê²°ê³„ì¸µ(fully connected layer)ë¡œ ì´ë£¨ì–´ì§„ ì‹ ê²½ë§, í¸í–¥ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í–‰ë ¬ ê³± ì—°ì‚°
  - ê° í™”ì‚´í‘œë§ˆë‹¤ì˜ ê°€ì¤‘ì¹˜ê°€ ì¡´ì¬, ì…ë ¥ì¸µ ë‰´ëŸ°ê³¼ì˜ ê°€ì¤‘í•©(weight sum)ì´ ì€ë‹‰ì¸µ ë‰´ëŸ°ì´ ë¨

```python
import numpy as np

c = np.array([[1, 0, 0, 0, 0 ,0 ,0]])
W = np.random.randn(7, 3)
layer = MatMul(W) # matmulì— ê°€ì¤‘ì¹˜ ì„¤ì •
h = layer.forward(c) #ìˆœì „íŒŒ ìˆ˜í–‰
print(h)
```

- ì½”ë“œ ì‹¤í–‰
  - ì™„ì „ì—°ê²°ê³„ì¸µì˜ ê³„ì‚°ì€ í–‰ë ¬ ê³±ìœ¼ë¡œ ìˆ˜í–‰
  - ë¯¸ë‹ˆë°°ì¹˜ ì²˜ë¦¬ë¥¼ ê³ ë ¤í•˜ì—¬ ì…ë ¥ë°ì´í„° cëŠ” 2ì°¨ì›ìœ¼ë¡œ ì§€ì • --> ìµœì´ˆì˜ ì°¨ì›(0ë²ˆì§¸ ì°¨ì›)ì— ê° ë°ì´í„°ë¥¼ ì €ì¥í•œ ê²ƒ
  - cì™€ Wì˜ í–‰ë ¬ê³±ì€ ê²°êµ­ ê°€ì¤‘ì¹˜ì˜ í–‰ë²¡í„° í•˜ë‚˜ë¥¼ ë½‘ì•„ë‚¸ ê²ƒê³¼ ê°™ìŒ; í•´ë‹¹ ìœ„ì¹˜ì˜ í–‰ë²¡í„°ê°€ **ì¶”ì¶œ**
  - <img src="https://imgur.com/GHQYpax.png" alt="https://imgur.com/GHQYpax.png" style="zoom:50%;" />



### 3.2 ë‹¨ìˆœí•œ word2vec

##### 3.2.1 CBOW ëª¨ë¸ì˜ ì¶”ë¡  ì²˜ë¦¬

- CBOW; continuous bag-of-words

  - context(ë§¥ë½)ìœ¼ë¡œë¶€í„° target(ì¤‘ì‹¬)ì„ ì¶”ì¸¡í•˜ëŠ” ìš©ë„ì˜ ì‹ ê²½ë§
  - context -> one-hot encoding -> ì…ë ¥ê°’ìœ¼ë¡œ ë°›ìŒ

- ë‰´ëŸ°(ëª¨ë¸)ì˜ ê´€ì ì—ì„œ CBOW

  - <img src="https://imgur.com/VCAoS4b.png" alt="https://imgur.com/VCAoS4b.png" style="zoom:50%;" />
    - Nê°œì˜ ì…ë ¥ì¸µ ~ ì™„ì „ì—°ê²°ê³„ì¸µ ~ ì€ë‹‰ì¸µ ~ ì™„ì „ì—°ê²°ê³„ì¸µ ~ ì¶œë ¥ì¸µ
    - ì—¬ëŸ¬ê°œì˜ ì…ë ¥ì¸µì¸ ê²½ìš° ì „ì²´ì˜ í‰ê· ì„ êµ¬í•œ ê°’ì´ ì€ë‹‰ì¸µ ë‰´ëŸ°ìœ¼ë¡œ íë¦„
    - ì¶œë ¥ì¸µ ë‰´ëŸ°ì€ ê° ë‹¨ì–´ì˜ Score(ì ìˆ˜)ë¥¼ ì¶œë ¥ -> ì—¬ê¸°ì— Softmax functionì„ ì ìš©í•˜ì—¬ í™•ë¥  ë„ì¶œ
  - ì…ë ¥ì¸µ ë‹¤ìŒì˜ ì™„ì „ì—°ê²°ê³„ì¸µì˜ ê°€ì¤‘ì¹˜ê°€ ë°”ë¡œ **ë‹¨ì–´ë¶„ì‚°í‘œí˜„**
    - <img src="https://imgur.com/3ShMAM6.png" alt="https://imgur.com/3ShMAM6.png" style="zoom:33%;" align="left" />
    - í•™ìŠµì„ ì§„í–‰í• ìˆ˜ë¡ ë§¥ë½ì—ì„œ ì¶œí˜„í•˜ëŠ” ë‹¨ì–´ë¥¼ ì˜ ì¶”ì¸¡í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë¶„ì‚°í‘œí˜„ë“¤ì´ ê°±ì‹ 
    - ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë‹¤ì°¨ì› ê³µê°„ì— ë²¡í„°í™”í•œ ê²ƒ
  - ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ì„ ì…ë ¥ì¸µì˜ ë‰´ëŸ°ìˆ˜ë³´ë‹¤ ì ê²Œí•˜ì—¬ í¬ì†Œë²¡í„°ë¥¼ ë°€ì§‘ë²¡í„°ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒì´ í•µì‹¬
    - ì€ë‹‰ì¸µì˜ ì •ë³´(ì¸ì½”ë”©) --> ì¶œë ¥ì¸µì˜ ì ìˆ˜(ë””ì½”ë”©)

- ê³„ì¸µ ê´€ì ì—ì„œì˜ CBOW

  - ![image-20191109221901428](https://imgur.com/U5Zypoe.png)
  -  Nê°œì˜ MatMul ê³„ì¸µ(ë§¥ë½ìœ¼ë¡œ ê³ ë ¤í•  ë‹¨ì–´ê°œìˆ˜ N) ~~ **ì™„ì „ì—°ê²°ê³„ì¸µ(weight sum) *0.5; ê°€ì¤‘ì¹˜ í‰ê· ** ~~ ì€ë‹‰ì¸µ ~~ MatMulê³„ì¸µ~~ ì¶œë ¥ì¸µ; ìµœì¢… score ì¶œë ¥

- CBOW ëª¨ë¸ ì¶”ë¡  ê³¼ì •

  ```python
  ### CBOW ëª¨ë¸ì˜ ì¶”ë¡  ì²˜ë¦¬
  
  # ìƒ˜í”Œ ë§¥ë½ ë°ì´í„°; one-hot encodingí•œ í¬ì†Œë²¡í„°
  c0 = np.array([1, 0, 0, 0, 0, 0, 0])
  c1 = np.array([0, 0, 1, 0, 0, 0, 0])
  
  #ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
  W_in = np.random.randn(7, 3)
  W_out = np.random.randn(3, 7)
  
  # MatMulê³„ì¸µ ìƒì„±; ë§¥ë½ì˜ ìˆ˜ ë§Œí¼ ìƒì„±í•¨(ì—¬ê¸°ì„œëŠ” 2ê°œ)
  in_layer0 = MatMul(W_in) # ì…ë ¥ì¸µì€ ê°€ì¤‘ì¹˜ W_inì„ ê³µìœ 
  in_layer1 = MatMul(W_in)
  out_layer = MatMul(W_out) # ì¶œë ¥ì¸µì˜ MatMul ë ˆì´ì–´ëŠ” 1ê°œë§Œ ìƒì„±
  
  #ìˆœì „íŒŒ
  h0 = in_layer0.forward(c0) # ì¤‘ê°„ë°ì´í„° ê³„ì‚°
  h1 = in_layer1.forward(c1)
  h = 0.5 * (h0 + h1) #í‰ê· 
  s = out_layer.forward(h) # ì¶œë ¥ì¸µ í†µê³¼í•˜ì—¬ score ë„ì¶œ
  
  print(s)
  ```

  - í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©ì¹˜ ì•ŠëŠ” ê°„ë‹¨í•œ êµ¬ì„±ì˜ ì‹ ê²½ë§
  - ì…ë ¥ì¸µì´ ì—¬ëŸ¬ê°œ(ê³ ë ¤í•  ë§¥ë½ì˜ ê°œìˆ˜ë§Œí¼) ì¡´ì¬í•˜ë©° ì…ë ¥ì¸µ ê°„ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ 



##### 3.2.2 CBOW ëª¨ë¸ì˜ í•™ìŠµ

- ì¶œë ¥ì¸µì˜ scoreì— softmax functionì„ ì ìš©í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜ -> í™•ë¥ ê³¼ ì •ë‹µ ë ˆì´ë¸”ë¡œë¶€í„° êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¥¼ êµ¬í•¨ -> ì†ì‹¤ë¡œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ---> ì˜¬ë°”ë¥¸ ì¤‘ì‹¬ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •
  - ë§¥ë½ì´ ì£¼ì–´ì¡Œì„ ë•Œ ì¤‘ì•™ë‹¨ì–´ë¡œ ì–´ë–¤ ê²ƒì´ ì¶œí˜„í•˜ëŠ” ì§€ ë‚˜íƒ€ë‚¸ ê²ƒ ==> ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ë‰´ëŸ°ì˜ ê°’ì´ ê°€ì¥ í¼
- ì£¼ì–´ì§€ëŠ” í•™ìŠµ ë§ë­‰ì¹˜ì— ë‚˜ì˜¤ëŠ” ë‹¨ì–´ ì¶œí˜„ íŒ¨í„´ì„ í•™ìŠµí•˜ë¯€ë¡œ ë§ë­‰ì¹˜ì— ë”°ë¼ ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ë„ ë‹¬ë¼ì§
- ![image-20191109225113344](https://imgur.com/US0W2Yq.png)
  - ì†Œí”„íŠ¸ë§¥ìŠ¤ ê³„ì¸µê³¼ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¥¼ Softmax with Lossë¼ëŠ” í•˜ë‚˜ì˜ ê³„ì¸µìœ¼ë¡œ êµ¬í˜„

##### 3.2.3 word2vecì˜ ê°€ì¤‘ì¹˜ì™€ ë¶„ì‚°í‘œí˜„

- ì…ë ¥ ì¸¡ì˜ ì™„ì „ì—°ê²°ê³„ì¸µ ê°€ì¤‘ì¹˜ì˜ ê° í–‰ì´ ê° ë‹¨ì–´ì˜ ë¶„ì‚°í‘œí˜„
- ì¶œë ¥ ì¸¡ì˜ ì™„ì „ì—°ê²°ê³„ì¸µ ê°€ì¤‘ì¹˜ì—ë„ ë‹¨ì–´ì˜ ì˜ë¯¸ê°€ ì¸ì½”ë”©ëœ ë²¡í„°ê°€ ì €ì¥ë˜ì–´ìˆìŒ
- ì¶œë ¥ì¸¡ ê°€ì¤‘ì¹˜ëŠ” ë¶„ì‚° í‘œí˜„ì´ ì—´ ë°©í–¥(ìˆ˜ì§ë°©í–¥)ìœ¼ë¡œ ì €ì¥
- ![image-20191109225817422](https://imgur.com/KwaGuHm.png)
- ë³´í†µ ì…ë ¥ ì¸¡ì˜ ê°€ì¤‘ì¹˜ë§Œì„ ìµœì¢… ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ìœ¼ë¡œì„œ ì´ìš©



### 3.3 í•™ìŠµ ë°ì´í„° ì¤€ë¹„

- "You say goodbye and I say hello"

##### 3.3.1 ë§¥ë½ê³¼ íƒ€ê¹ƒ

- ì…ë ¥; context(ë§¥ë½) / ì •ë‹µ ë ˆì´ë¸”; target(íƒ€ê¹ƒ)

  - => ì‹ ê²½ë§ì— ë§¥ë½ì„ ì…ë ¥í–ˆì„ ë•Œ íƒ€ê¹ƒì´ ì¶œí˜„í•  í™•ë¥ ì„ ë†’ì´ë„ë¡ í•™ìŠµ

- ë§¥ë½ê³¼ íƒ€ê¹ƒì„ ë§Œë“œëŠ” ê²ƒì„ ë§ë­‰ì¹˜ ì•ˆì˜ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•´ ìˆ˜í–‰í•¨

  - ![page132image61531280.jpg](https://imgur.com/FYhpGgN.jpg) 
  - ë§¥ë½:íƒ€ê¹ƒì€ ë‹¤ ëŒ€ ì¼ ê´€ê³„
  - ë§¥ë½-íƒ€ê¹ƒìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê° í–‰ì´ ì‹ ê²½ë§ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©ë¨

- ë§ë­‰ì¹˜ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ IDë¡œ ë³€í™˜(ê³ ìœ ê°’ ë¶€ì—¬)

  - ```python
    ### ë§ë­‰ì¹˜ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ IDë¡œ ë³€í™˜
    
    def preprocess(text):
        text = text.lower()
        text = text.replace('.', ' .')
        words = text.split(' ')
    
        word_to_id = {}
        id_to_word = {}
        
        for word in words:
            if word not in word_to_id:
                new_id = len(word_to_id)
                word_to_id[word] = new_id
                id_to_word[new_id] = word
    
        corpus = np.array([word_to_id[w] for w in words])
    
        return corpus, word_to_id, id_to_word
    ```

  - ```python
    text = 'You say goodbye and I say hello.'
    corpus, word_to_id, id_to_word = preprocess(text)
    print(corpus)
    print(id_to_word)
    ```

  - ë§¥ë½==2ì°¨ì› í–‰ë ¬ -> 0ë²ˆì§¸ ì°¨ì›ì—ëŠ” ê° ë§¥ë½ ë°ì´í„°ê°€ ì €ì¥

- contextsì™€ target ìƒì„±

  - ```python
    ### ë§¥ë½ê³¼ íƒ€ê¹ƒì„ ë§Œë“œëŠ” í•¨ìˆ˜
    
    def create_contexts_target(corpus, window_size=1):
      # ë‹¨ì–´IDì˜ ë°°ì—´(corpus), ë§¥ë½ì˜ ìœˆë„ìš°í¬ê¸°(window_size)
      
        # target ê³„ì‚°
        target = corpus[window_size:-window_size]
        contexts = []
        
        # contexts ê³„ì‚°
        for idx in range(window_size, len(corpus)-window_size):
            cs=[]
            for t in range(-window_size, window_size + 1):
                if t == 0:
                    continue
                cs.append(corpus[idx + t])
            contexts.append(cs)
        # ë§¥ë½ê³¼ íƒ€ê¹ƒì„ numpy ë‹¤ì°¨ì› ë°°ì—´ë¡œ return      
        return np.array(contexts), np.array(target)
    ```

    ```python
    contexts, target = create_contexts_target(corpus)
    print(contexts)
    print(target)
    ```

  - CBOW ëª¨ë¸ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©

##### 3.3.2 ì›í•« í‘œí˜„ìœ¼ë¡œ ë³€í™˜

- <img src="https://imgur.com/pfyMo5P.png" alt="https://imgur.com/pfyMo5P.png" style="zoom:50%;" />
  - ë§¥ë½ê³¼ íƒ€ê¹ƒì„ ë‹¨ì–´IDì—ì„œ ì›í•«í‘œí˜„ìœ¼ë¡œ ë³€í™˜
  - ë§¥ë½(6,2) --> ì›í•« í‘œí˜„; ë‹¤ì°¨ì›ë°°ì—´ (6,2,7)

- ë‹¨ì–´ IDëª©ë¡ê³¼ ì–´íœ˜ ìˆ˜ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë°›ì•„ì„œ ì›í•«ì¸ì½”ë”© ìˆ˜í–‰

  - ```python
    def convert_one_hot(corpus, vocab_size):
        N = corpus.shape[0]
    
        if corpus.ndim == 1:
            one_hot = np.zeros((N, vocab_size), dtype=np.int32)
            for idx, word_id in enumerate(corpus):
                one_hot[idx, word_id] = 1
    
        elif corpus.ndim == 2:
            C = corpus.shape[1]
            one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)
            for idx_0, word_ids in enumerate(corpus):
                for idx_1, word_id in enumerate(word_ids):
                    one_hot[idx_0, idx_1, word_id] = 1
    
        return one_hot
    ```

    ```python
    ### ì›í•« í‘œí˜„ìœ¼ë¡œ ë³€í™˜
    
    text = 'You say goodbye and I say hello.'
    corpus, word_to_id, id_to_word = preprocess(text)
    
    contexts, target = create_contexts_target(corpus, window_size=1)
    
    vocab_size = len(word_to_id)
    target = convert_one_hot(target, vocab_size)
    contexts = convert_one_hot(contexts, vocab_size)
    ```

    ```python
    print(contexts)
    print(target)
    ```



### 3.4 CBOW ëª¨ë¸ êµ¬í˜„

- SimpleCBOW êµ¬í˜„

  - ```python
    ### 3.4 CBOW ëª¨ë¸ êµ¬í˜„
    class SimpleCBOW:
        def __init__(self, vocab_size, hidden_size):
            V, H = vocab_size, hidden_size
            # ì–´íœ˜ìˆ˜(vocab_size)ì™€ ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ìˆ˜(hidden_size)ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ
            
            # 1) ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”(ê°ê° ì‘ì€ ë¬´ì‘ìœ„ê°’ìœ¼ë¡œ ì´ˆê¸°í™”)
            # numpyë°°ì—´ì˜ ë°ì´í„°íƒ€ì…ì„ astype('f')ë¡œ ì§€ì •; 32bit ë¶€ë™ì†Œìˆ˜ì ìˆ˜ë¡œ ì´ˆê¸°í™” í•œ ê²ƒ
            W_in = 0.01 * np.random.randn(V, H).astype('f')
            W_out = 0.01 * np.random.randn(H, V).astype('f')
    
            # 2) ê³„ì¸µ ìƒì„±
            # ë§¥ë½ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ì˜ ìˆ˜(window_size)ë§Œí¼ ì…ë ¥ì¸µì„ ìŒ“ì•„ì•¼í•¨
            # ê° ì…ë ¥ì¸µì€ ê°™ì€ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©
            self.in_layer0 = MatMul(W_in)
            self.in_layer1 = MatMul(W_in)
            self.out_layer = MatMul(W_out)
            self.loss_layer = SoftmaxWithLoss()
    
            # 3) ëª¨ë“  ê°€ì¤‘ì¹˜ì™€ ê¸°ìš¸ê¸°ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ëª¨ìŒ
            layers = [self.in_layer0, self.in_layer1, self.out_layer]
            self.params, self.grads = [], []
            for layer in layers:
                self.params += layer.params
                self.grads += layer.grads
    
            # 4) ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ë‹¨ì–´ì˜ ë¶„ì‚°í‘œí˜„ì„ ì €ì¥
            self.word_vecs = W_in
    
        # ìˆœì „íŒŒ ë©”ì†Œë“œ; ë§¥ë½ê³¼ íƒ€ê¹ƒì„ ë°›ì•„ ì†ì‹¤ì„ ë°˜í™˜
        def forward(self, contexts, target):
            #ë§¥ë½ì€ 3ì°¨ì› numpy arr; ex) (6,2,7)
            #0ë²ˆì§¸ ì°¨ì›ì˜ ì›ì†Œ ìˆ˜ëŠ” ë¯¸ë‹ˆë°°ì¹˜ì˜ ìˆ˜ë§Œí¼, 1ë²ˆì§¸ ì°¨ì›ì˜ ì›ì†Œ ìˆ˜ëŠ” ë§¥ë½ì˜ ìœˆë„ìš°í¬ê¸°, 2ë²ˆì§¸ ì°¨ì›ì€ ì›-í•« ë²¡í„°
            #targetì˜ í˜•ìƒì€ 2ì°¨ì›; ex) (6,7)
            h0 = self.in_layer0.forward(contexts[:, 0])
            h1 = self.in_layer1.forward(contexts[:, 1])
            h = (h0 + h1) * 0.5
            score = self.out_layer.forward(h)
            loss = self.loss_layer.forward(score, target)
            return loss
    
        # ì—­ì „íŒŒ ë©”ì†Œë“œ
        def backward(self, dout=1):
            # 1ì—ì„œ ì‹œì‘í•´ì„œ softmax with lossê³„ì¸µì— ì…ë ¥
            ds = self.loss_layer.backward(dout)
            # dsë¥¼ MatMulê³„ì¸µì— ì…ë ¥
            da = self.out_layer.backward(ds)
            da *= 0.5 
            # 'x' ì—­ì „íŒŒëŠ” ìˆœì „íŒŒì˜ ì…ë ¥ì„ ì„œë¡œ ë°”ê¿” ê¸°ìš¸ê¸°ì— ê³±í•¨
            # '+'ì—­ì „íŒŒëŠ” ê¸°ìš¸ê¸°ë¥¼ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚´
            self.in_layer1.backward(da)
            self.in_layer0.backward(da)
            return None
    ```

- <img src="https://imgur.com/xTAhesh.png" alt="https://imgur.com/xTAhesh.png" style="zoom:50%;" />
  - 1ì—ì„œ ì‹œì‘í•´ì„œ softmax with lossê³„ì¸µì— ì…ë ¥ -> ds -> ì¶œë ¥ MatMulê³„ì¸µìœ¼ë¡œ ì…ë ¥ -> ê³±ì…ˆ; ìˆœì „íŒŒ ì‹œ ì…ë ¥ì„ ë°”ê¿” ê¸°ìš¸ê¸°ì— ê³±í•¨ -> ë§ì…ˆ; ê¸°ìš¸ê¸°ë¥¼ ê·¸ëŒ€ë¡œ í†µê³¼

##### 3.4.1 í•™ìŠµ ì½”ë“œ êµ¬í˜„

- í•™ìŠµ ë°ì´í„°ë¥¼ ì‹ ê²½ë§ì— ì…ë ¥ -> ê¸°ìš¸ê¸°ë¥¼ êµ¬í•¨ -> ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ìˆœì„œëŒ€ë¡œ ê°±ì‹ 

- ```python
  import matplotlib.pyplot as plt
  
  window_size = 1
  hidden_size = 5
  batch_size = 3
  max_epoch = 1000
  
  text = 'You say goodbye and I say hello.'
  corpus, word_to_id, id_to_word = preprocess(text)
  
  vocab_size = len(word_to_id)
  contexts, target = create_contexts_target(corpus, window_size)
  target = convert_one_hot(target, vocab_size)
  contexts = convert_one_hot(contexts, vocab_size)
  
  model = SimpleCBOW(vocab_size, hidden_size)
  optimizer = Adam() #Adamìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
  trainer = Trainer(model, optimizer)
  
  trainer.fit(contexts, target, max_epoch, batch_size)
  trainer.plot()
  plt.show()
  ```

- ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ ì¡°íšŒ

  - ```python
    # word_vecs; ê° í–‰ì— ëŒ€ì‘í•˜ëŠ” ë‹¨ì–´IDì˜ ë¶„ì‚°í‘œí˜„(ì…ë ¥ MatMul ê³„ì¸µì˜ ê°€ì¤‘ì¹˜)ì´ ì €ì¥ë˜ì–´ìˆìŒ
    word_vecs = model.word_vecs
    for word_id, word in id_to_word.items():
        print(word, word_vecs[word_id])
    ```

- ë§ë­‰ì¹˜ê°€ ì‘ê¸° ë•Œë¬¸ì— ì¢‹ì€ ê²°ê³¼ëŠ” X



### 3.5 word2vec ë³´ì¶©

##### 3.5.1 CBOW ëª¨ë¸ê³¼ í™•ë¥ 

- ë™ì‹œí™•ë¥  P(A,B); Aì™€ Bê°€ ë™ì‹œì— ì¼ì–´ë‚  í™•ë¥ 
- ì‚¬í›„í™•ë¥  P(A|B); Bë¼ëŠ” ì •ë³´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ Aê°€ ì¼ì–´ë‚  í™•ë¥ 
- <img src="https://imgur.com/o1gaREB.png" alt="https://imgur.com/o1gaREB.png" style="zoom:50%;" align="left"/>
  - ë§ë­‰ì¹˜ë¥¼ ë‹¨ì–´ ì‹œí€€ìŠ¤ë¡œ í‘œê¸°, të²ˆì§¸ ë‹¨ì–´ì— ëŒ€í•´ ìœˆë„ìš° í¬ê¸°ê°€ 1ì¸ ë§¥ë½
- <img src="https://imgur.com/4KQOHth.png" alt="https://imgur.com/4KQOHth.png" style="zoom:50%;" align="left" />
  - Wt-1ê³¼ Wt+1ì´ ì£¼ì–´ì¡Œì„ ë•Œ íƒ€ê¹ƒì´ Wtê°€ ë  í™•ë¥ 
  - CBOWê°€ ëª¨ë¸ë§ í•˜ê³  ìˆëŠ” ìˆ˜ì‹
- <img src="https://imgur.com/hhSdKrV.png" alt="https://imgur.com/hhSdKrV.png" style="zoom:50%;" align="left"/>
  - Wtì— í•´ë‹¹í•˜ëŠ” ì›ì†Œë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì¸ í¬ì†Œë²¡í„°ì´ë¯€ë¡œ Wtì´ì™¸ì˜ ì¼ì´ ì¼ì–´ë‚  ê²½ìš°ëŠ” ì›í•« ë ˆì´ë¸” ìš”ì†Œê°€ 0
  - => ë‹¨ìˆœíˆ í™•ë¥ ì— **ìŒì˜ ë¡œê·¸ ê°€ëŠ¥ë„** ë¥¼ ì·¨í•¨; logë¥¼ ì·¨í•˜ê³  ë§ˆì´ë„ˆìŠ¤ë¥¼ ë¶™ì„
  - ë§ë­‰ì¹˜ ì „ì²´ë¡œ í™•ì¥ í–ˆì„ ë•ŒëŠ” ì „ì²´ í•©ì˜ í‰ê· 
  - ì´ ê°’ì„ ê°€ëŠ¥í•œ ì‘ê²Œ ë§Œë“œëŠ” ê²ƒì´ í•™ìŠµì˜ ëª©ì , W(ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜)ëŠ” ë‹¨ì–´ì˜ ë¶„ì‚°í‘œí˜„(ë°€ì§‘ë²¡í„°)



##### 3.5.2 skip-gram ëª¨ë¸

- CBOWì—ì„œ ë‹¤ë£¨ëŠ” ë§¥ë½ê³¼ íƒ€ê¹ƒì„ ì—­ì „ì‹œí‚¨ ëª¨ë¸
- <img src="https://imgur.com/OgGt0ls.png" alt="https://imgur.com/OgGt0ls" style="zoom:50%;" />
  - Skip-gramì€ íƒ€ê¹ƒìœ¼ë¡œë¶€í„° ì£¼ë³€ì˜ ì—¬ëŸ¬ ë‹¨ì–´(ë§¥ë½)ì„ ì¶”ì¸¡
- <img src="https://imgur.com/PpjpvmO.png" alt="https://imgur.com/PpjpvmO" style="zoom:50%;" />
  - ì…ë ¥ì¸µì€ 1ê°œ, ì¶œë ¥ì¸µì€ ë§¥ë½ì˜ ìˆ˜ ë§Œí¼ ì¡´ì¬ --> ê° ì¶œë ¥ì¸µì—ì„œ ê°œë³„ì ìœ¼ë¡œ ì†ì‹¤ì„ êµ¬í•´ í•©ì‚°í•œ ê°’ì´ ìµœì¢… ì†ì‹¤ê°’
- $P(w_{t-1}, w_{t+1}|w_t)$
  - $w_t$ë¡œë¶€í„° $w_{t-1}, w_{t+1}$ì´ ë™ì‹œì— ì¼ì–´ë‚  í™•ë¥ ì„ ì¶”ì¸¡--> skip-gramì´ ëª¨ë¸ë§í•˜ëŠ” ìˆ˜ì‹
- $P(w_{t-1}, w_{t+1}|w_t)=P(w_{t-1}|w_t)P(w_{t+1}|w_t)$
  - ë§¥ë½ì˜ ë‹¨ì–´ ì‚¬ì´ì— ê´€ë ¨ì„±ì´ ì—†ë‹¤ê³  ê°€ì •(ì¡°ê±´ë¶€ ë…ë¦½)í•˜ì—¬ ë¶„í•´
- $$\begin{align}
    L&=-logP(w_{t-1},w_{t+1}|w_t) \\
     &=-logP(w_{t-1}|w_t)(w_{t+1}|w_t) \\
     &=-(logP(w_{t-1}w_t)+logP(w_{t+1}|w_t))
    \end{align}$$
  - êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ì— ì ìš©í•˜ì—¬ ì†ì‹¤í•¨ìˆ˜ ìœ ë„
  - logxy = loge + logyë¼ëŠ” ë¡œê·¸ì˜ ì„±ì§ˆì„ í™œìš©
  - ë§¥ë½ë³„ ì†ì‹¤ì„ êµ¬í•œ ë‹¤ìŒ ëª¨ë‘ ë”í•¨
- $L=-\frac{1}{T}\displaystyle \sum_{t=1}^T (logP(w_{t-1}|w_t)+logP(w_{t+1}|w_t))$
  - ë§ë­‰ì¹˜ ì „ì²´ë¡œ í™•ì¥(í•©ì‚° í›„ í‰ê· )
  - ëª¨ë¸ì˜ ë§¥ë½ ìˆ˜ ë§Œí¼ ì¶”ì¸¡í•˜ë¯€ë¡œ ì†ì‹¤í•¨ìˆ˜ëŠ” ê° ë§¥ë½ì—ì„œ êµ¬í•œ ì†ì‹¤ì˜ ì´í•©
- ë‹¨ì–´ ë¶„ì‚° í‘œí˜„ì˜ ì •ë°€ë„, ì €ë¹ˆë„ ë‹¨ì–´ë‚˜ ìœ ì¶”ë¬¸ì œ ì„±ëŠ¥ë©´ì—ì„œ skip-gram ëª¨ë¸ì˜ ê²°ê³¼ê°€ ë” ì¢‹ì§€ë§Œ ì†ì‹¤ì„ ë§¥ë½ ìˆ˜ ë§Œí¼ êµ¬í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ê³„ì‚° ë¹„ìš©ì´ ì»¤ í•™ìŠµ ì†ë„ê°€ ëŠë¦¼
- ë°˜ë©´ CBOWëª¨ë¸ì€ í•™ìŠµ ì†ë„ê°€ ë¹ ë¦„



##### 3.5.3 í†µê³„ ê¸°ë°˜ vs. ì¶”ë¡  ê¸°ë°˜

- í†µê³„ê¸°ë°˜ê¸°ë²•
  - ë§ë­‰ì¹˜ ì „ì²´ í†µê³„ë¡œë¶€í„° 1íšŒ í•™ìŠµí•˜ì—¬ ë‹¨ì–´ì˜ ë¶„ì‚°í‘œí˜„ì„ ì–»ìŒ
  - ìƒˆë¡œìš´ ë‹¨ì–´ê°€ ì¶”ê°€ë˜ì–´ ë‹¨ì–´ì˜ ë¶„ì‚°í‘œí˜„ì„ ê°±ì‹ í•´ì•¼í•˜ëŠ” ê²½ìš° ì²˜ìŒë¶€í„° ê³„ì‚°í•´ì•¼í•¨
  - ë‹¨ì–´ì˜ ìœ ì‚¬ì„±ì´ ì¸ì½”ë”©
- ì¶”ë¡ ê¸°ë°˜ê¸°ë²•
  - ë§ë­‰ì¹˜ë¥¼ ì¼ë¶€ë¶„ì”© ì—¬ëŸ¬ë²ˆ ë³´ë©´ì„œ í•™ìŠµ(ë¯¸ë‹ˆë°°ì¹˜)
  - ìƒˆë¡œìš´ ë‹¨ì–´ê°€ ì¶”ê°€ë˜ì—ˆì„ ë•Œ í•™ìŠµí•˜ë˜ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°ê°’ìœ¼ë¡œ ì‚¬ìš©í•´ ì¬í•™ìŠµ ê°€ëŠ¥ -> ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê°±ì‹ 
  - ë‹¨ì–´ì˜ ìœ ì‚¬ì„± ë° ë‹¨ì–´ ê°„ ë³µì¡í•œ íŒ¨í„´ ì¸ì½”ë”©
- ë‘ ê¸°ë²•ì€ ì„±ëŠ¥ë©´ì—ì„œ í° ì°¨ì´ëŠ” ì—†ìŒ
- Skip-gramê³¼ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì„ ì´ìš©í•œ ëª¨ë¸ì€ ë§ë­‰ì¹˜ì˜ ë™ì‹œë°œìƒ í–‰ë ¬ì— íŠ¹ìˆ˜ í–‰ë ¬ë¶„í•´ë¥¼ ì ìš©í•œ ê²ƒ --> í†µê³„ê¸°ë°˜ê¸°ë²•ê³¼ ì¶”ë¡ ê¸°ë°˜ê¸°ë²•ì´ ì„œë¡œ ì—°ê´€ ìˆìŒ
- GloVe; í†µê³„ê¸°ë°˜ê¸°ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ ë§ë­‰ì¹˜ ì „ì²´ í†µê³„ì •ë³´ë¥¼ ì†ì‹¤í•¨ìˆ˜ì— ë„ì…, ì¶”ë¡ ê¸°ë°˜ê¸°ë²•ì˜ ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµì„ ìˆ˜í–‰



<img src="https://imgur.com/zkyMH9I.png" alt="https://imgur.com/zkyMH9I" style="zoom:50%;" />





### chapter 4. word2vec ì†ë„ ê°œì„ 

---

- SimpleCBOWëª¨ë¸ì€ ì–´íœ˜ ìˆ˜ê°€ ë§ì•„ì§€ë©´ ê³„ì‚°ëŸ‰ë„ ì»¤ì ¸ ê³„ì‚°ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼ -> ì†ë„ ê°œì„  í•„ìš”

  1ï¸âƒ£ Embedding ê³„ì¸µ ë„ì…

  2ï¸âƒ£ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì´ë¼ëŠ” ì†ì‹¤ í•¨ìˆ˜ ë„ì…

  3ï¸âƒ£ PTB ë°ì´í„°ì…‹(ì‹¤ìš©ì  í¬ê¸°ì˜ ë§ë­‰ì¹˜)ë¡œ í•™ìŠµì„ ìˆ˜í–‰í•´ ì–»ì€ ë‹¨ì–´ ë¶„ì‚°í‘œí˜„ì„ í‰ê°€



### 4.1 word2vec ê°œì„  (1)

- <img src="https://imgur.com/ub8yRCh.png" alt="https://imgur.com/ub8yRCh" style="zoom:50%;" />
  - ì–´íœ˜ 100ë§Œê°œ, ì€ë‹‰ì¸µ ë‰´ëŸ°ì´ 100ê°œì¸ CBOW ëª¨ë¸
  - ì…ë ¥ì¸µ ì›-í•« í‘œí˜„ê³¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ê³± ê³„ì‚°ì—ì„œ ë³‘ëª© ë°œìƒ
    - ì›-í•« í‘œí˜„ì´ í¬ì†Œë²¡í„°ì´ë¯€ë¡œ ì–´íœ˜ 100ë§Œê°œì— ë”°ë¼ 100ë§Œê°œì˜ ë²¡í„°ê°€ ë¨ -> ë©”ëª¨ë¦¬ ë¦¬ì†ŒìŠ¤ ğŸ¤·â€â™€ï¸ -> ê³± ì—°ì‚°ì—ì„œ ë§ì€ ë¦¬ì†ŒìŠ¤ê°€ ì†Œëª¨ë¨
    - Embedding  ê³„ì¸µ ë„ì…ìœ¼ë¡œ í•´ê²°
  - ì€ë‹‰ì¸µê³¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ê³±, softmaxê³„ì¸µ ê³„ì‚°ì—ì„œ ë³‘ëª© ë°œìƒ
    - ì€ë‹‰ì¸µ*ê°€ì¤‘ì¹˜í–‰ë ¬ì—ì„œë„ ë¦¬ì†ŒìŠ¤ê°€ ë§ì´ ì¡íˆëŠ”ë° softmax ê³„ì¸µì—ì„œ ë‹¤ë£¨ëŠ” ì–´íœ˜ê°€ ë§ì•„ì§ì— ë”°ë¼ ê³„ì‚°ëŸ‰ì´ ì¦ê°€
    - ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì„ ë„ì…í•˜ì—¬ í•´ê²°

##### 4.1.1 Embedding ê³„ì¸µ

-  ì…ë ¥ ì¸¡ì˜ ì—°ì‚°ì—ì„œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì€ **í–‰ë ¬ì˜ íŠ¹ì • í–‰ì„ ì¶”ì¶œ**í•˜ëŠ” ê²ƒ
  - => ì›-í•« ë³€í™˜ ë° MatMulê³„ì¸µì˜ í–‰ë ¬ ê³± ì—°ì‚°ì„ ì‚­ì œí•´ë„ ë¬´ë°©
- ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¡œë¶€í„° ë‹¨ì–´IDì— í•´ë‹¹í•˜ëŠ” í–‰(ë²¡í„°)ë¥¼ ì¶”ì¶œí•˜ëŠ” ê³„ì¸µìœ¼ë¡œ ëŒ€ì²´(Embedding ê³„ì¸µ) -> ë‹¨ì–´ ì„ë² ë”©(ë¶„ì‚°í‘œí˜„)ì„ ì €ì¥

##### 4.1.2 Embedding ê³„ì¸µ êµ¬í˜„

- ndim==2, numpy arrì˜ ê²½ìš° W[i]ë¡œ íŠ¹ì •í–‰ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ ì´ìš©

  - ```python
    class Embedding:
        def __init__(self, W):
            self.params = [W]
            self.grads = [np.zeros_like(W)]
            self.idx = None
    
        def forward(self, idx):
            W, = self.params
            self.idx = idx
            out = W[idx] # ë‹¨ìˆœíˆ ê°€ì¤‘ì¹˜ì˜ íŠ¹ì •í–‰ë§Œ ì¶”ì¶œ
            return out
    
        def backward(self, dout):
            dW, = self.grads 
            dW[...] = 0 # dWì˜ í˜•ìƒì„ ìœ ì§€í•œ ì±„ ì›ì†Œë¥¼ 0ìœ¼ë¡œ ë®ì–´ì”€
            np.add.at(dW, self.idx, dout)# ì•ì¸µì˜ ê¸°ìš¸ê¸°ë¥¼ ê°€ì¤‘ì¹˜ ê¸°ìš¸ê¸° dWì˜ íŠ¹ì •í–‰ì— ì„¤ì •
            return None
    ```

  - <img src="https://imgur.com/hnGYeKF.jpg" alt="https://imgur.com/hnGYeKF.jpg" style="zoom:50%;" /> 

  - idx ë°°ì—´ì˜ ì›ì†Œ ì¤‘ í–‰ë²ˆí˜¸ê°€ ê°™ì€ ì›ì†Œê°€ ìˆëŠ” ê²½ìš°-> í• ë‹¹ì´ ì•„ë‹Œ ë”í•˜ê¸°ê°€ í•„ìš”

    - ```python
      def backward(self, dout):
      		dW, = self.grads
          dW[...] = 0
          
          for i word_id in enumerate(self.idx):
            	dW[word_id] += dout[i]
          #np.add.at(dW, self.idx, dout) #numpyì˜ ë‚´ì¥ë©”ì†Œë“œ; ì²˜ë¦¬ì†ë„ ë¹ ë¦„
          
          return None
      ```



### 4.2 word2vec ê°œì„  (2)

##### 4.2.1 ì€ë‹‰ì¸µ ì´í›„ ê³„ì‚°ì˜ ë¬¸ì œì 

- ê±°ëŒ€í•œ í–‰ë ¬ ê³„ì‚°ìœ¼ë¡œ ì¸í•œ ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ë¬¸ì œ
  - ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ê³¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ê³±
  - Softmax ê³„ì¸µì˜ ê³„ì‚°
    - $y_k = \frac{exp(s_k)}{\displaystyle\sum_{i=1}^{1000000} exp(s_i)}$
    - ë¶„ëª¨ì˜ ê°’ì„ ì–»ê¸° ìœ„í•´ expê³„ì‚°ì„ 100ë§Œë²ˆ ìˆ˜í–‰í•´ì•¼í•˜ëŠ” êµ¬ì¡°
- ê°€ë²¼ìš´ ê³„ì‚°ì´ í•„ìš”



##### 4.2.2 ë‹¤ì¤‘ë¶„ë¥˜ì—ì„œ ì´ì§„ë¶„ë¥˜ë¡œ

- ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§
  - ë‹¤ì¤‘ë¶„ë¥˜(multi-classification)ì„ ì´ì§„ë¶„ë¥˜(binary classification)ë¡œ ê·¼ì‚¬í•˜ëŠ” ê²ƒ
  - <img src="https://imgur.com/HWYU0Ht.png" alt="https://imgur.com/HWYU0Ht" style="zoom:50%;" />
  - ì¶œë ¥ì¸µì— ë‰´ëŸ°ì„ í•˜ë‚˜ë§Œ ì¤€ë¹„í•˜ì—¬ íƒ€ê¹ƒ ë‹¨ì–´ì˜ scoreë§Œ êµ¬í•˜ë„ë¡ í•¨
  - ì€ë‹‰ì¸µê³¼ ì¶œë ¥ ì¸¡ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ë‚´ì (dot-product)ëŠ” íƒ€ê¹ƒì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë²¡í„°ë§Œ ì¶”ì¶œ -> ë‹¨ì–´ë²¡í„°ì™€ ì€ë‹‰ì¸µ ë‰´ëŸ°ê³¼ì˜ ë‚´ì  ê³„ì‚°ë§Œ ìˆ˜í–‰
- <img src="https://imgur.com/PsnIGNm.png" alt="https://imgur.com/PsnIGNm" style="zoom:50%;" />
  - ê° ë‹¨ì–´ì˜ ê³ ìœ ê°’ì˜ ë‹¨ì–´ë²¡í„°ê°€ ì—´ë¡œ ì €ì¥ë˜ì–´ ìˆê³  íƒ€ê¹ƒ ë‹¨ì–´ë²¡í„°ë¥¼ ì¶”ì¶œí•´ì„œ ì€ë‹‰ì¸µë‰´ëŸ°ê³¼ì˜ ë‚´ì ì„ êµ¬í•¨ -> ìµœì¢…ì ìˆ˜ -> ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ -> í™•ë¥ ë³€í™˜
  - => íƒ€ê¹ƒ ë‹¨ì–´ í•˜ë‚˜ì— ì£¼ëª©í•´ ê·¸ ì ìˆ˜ë§Œ ê³„ì‚°í•˜ëŠ” ê²ƒì´ í•µì‹¬



##### 4.2.3 ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì™€ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨

- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¡œ í™•ë¥ ë¡œ ë³€í™˜í•˜ê³  êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¡œ ì†ì‹¤í•¨ìˆ˜ ì‚¬ìš©

  > cf) ë‹¤ì¤‘ ë¶„ë¥˜ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ + êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨
  >
  > â€‹     ì´ì§„ ë¶„ë¥˜ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ + êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨

- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜

  - <img src="https://imgur.com/gudh9Lf.png" alt="https://imgur.com/gudh9Lf" style="zoom:50%;" />

- ì‹œê·¸ëª¨ì´ë“œ ì¶œë ¥ ê²°ê³¼ í™•ë¥  yë¡œë¶€í„° êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¥¼ ì ìš©í•˜ì—¬ ì†ì‹¤ê°’ì„ êµ¬í•¨

- <img src="https://imgur.com/JEWf1rW.png" alt="https://imgur.com/JEWf1rW" style="zoom:50%;" />

  - y == ì‹ ê²½ë§ì´ ì¶œë ¥í•œ í™•ë¥  / t == ì •ë‹µ ë ˆì´ë¸” -> y-t == ì¶”ë¡ ê²°ê³¼ì—ì„œ ì •ë‹µë ˆì´ë¸”ì„ ëº€ ê°’ 
  - ì˜¤ì°¨ê°€ í¬ë©´ í¬ê²Œ í•™ìŠµ, ì‘ìœ¼ë©´ ì‘ê²Œ í•™ìŠµ
  - > **cf; $\frac{\partial L}{\partial x}&=& y-tâ€‹$ê°€ ìœ ë„ë˜ëŠ” ê³¼ì •**
    > $$\begin{array}{lcl}\frac{\partial L}{\partial x}&=&\frac{\partial L}{\partial y}\frac{\partial y}{\partial x} \\\frac{\partial L}{\partial y}&=&-\frac{t}{y}+\frac{1-t}{1-y}= \frac{y-t}{y(1-y)} \\\frac{\partial y}{\partial x}&=&y(1-y) \\\\\therefore \frac{\partial L}{\partial x}&=& y-tâ€‹\end{array}$$


##### 4.2.4 ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ì´ì§„ ë¶„ë¥˜ë¡œ(êµ¬í˜„)

- <img src="https://imgur.com/bQ99S1f.png" alt="https://imgur.com/bQ99S1f" style="zoom:50%;" />

- Embedding ê³„ì¸µê³¼ dot ì—°ì‚° ì²˜ë¦¬ë¥¼ í•©ì³ Embedding Dot ê³„ì¸µ ë„ì…

  - <img src="https://imgur.com/MNbTzWy.png" alt="https://imgur.com/MNbTzWy" style="zoom:50%;" />

  - ```python
    # dot product(ë‚´ì )ê³¼ embedding ê³„ì¸µì„ í•©ì¹œ ê³„ì¸µ; ì€ë‹‰ì¸µ ì´í›„ ì²˜ë¦¬ë¥¼ í•œë²ˆì— ìˆ˜í–‰
    class EmbeddingDot:
        def __init__(self, W):
            self.embed = Embedding(W)   # embeddingê³„ì¸µ
            self.params = self.embed.params # ë§¤ê°œë³€ìˆ˜
            self.grads = self.embed.grads   # ê¸°ìš¸ê¸°
            self.cache = None   # ìˆœì „íŒŒì‹œì˜ ê³„ì‚°ê²°ê³¼ë¥¼ ì ì‹œ ìœ ì§€í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
    
        # ìˆœì „íŒŒ; ì€ë‹‰ì¸µ ë‰´ëŸ°(h), ë‹¨ì–´ IDì˜ ë„˜íŒŒì´ë°°ì—´(idx)
        def forward(self, h, idx):
            # idx: ë‹¨ì–´IDì˜ ë°°ì—´ -> í•œêº¼ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ë¯¸ë‹ˆë°°ì¹˜ ì²˜ë¦¬ë¥¼ ê°€ì •í•˜ì˜€ìŒ
            target_W = self.embed.forward(idx)
            out = np.sum(target_W * h, axis=1) #ë‚´ì  ê³„ì‚°, í–‰ì„ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜í–‰
    
            self.cache = (h, target_W)
            return out
    
        def backward(self, dout):
            h, target_W = self.cache # ìˆœì „íŒŒ ê³„ì‚°ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜´
            dout = dout.reshape(dout.shape[0], 1)
    
            dtarget_W = dout * h
            self.embed.backward(dtarget_W)
            dh = dout * target_W
            return dh
    ```



##### 4.2.5 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§

- ê¸°ì¡´ì˜ ë°©ë²•ì€ ê¸ì •ì ì¸ ì˜ˆë§Œ í•™ìŠµí•˜ê³  ë¶€ì •ì ì¸ ì˜ˆ(íƒ€ê¹ƒ ì™¸ì˜ ë‹¨ì–´)ì— ëŒ€í•œ ì§€ì‹ì€ X
- íƒ€ê¹ƒë‹¨ì–´ì— ëŒ€í•´ì„œëŠ” sigmoid ì¶œë ¥ê°’ì„ 1ì— ê°€ê¹ê²Œ, íƒ€ê¹ƒ ì™¸ ë‹¨ì–´ì— ëŒ€í•´ì„œëŠ” ì¶œë ¥ê°’ì„ 0ì— ê°€ê¹ê²Œ ë§Œë“¤ì–´ì•¼í•¨
  - <img src="https://imgur.com/dY6xW7T.png" alt="https://imgur.com/dY6xW7T" style="zoom:50%;" />
- ëª¨ë“  ë¶€ì •ì  ì˜ˆë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ë©´ ì–´íœ˜ ìˆ˜ê°€ ëŠ˜ì–´ë‚˜ê²Œ ë¨ -> ë‹¹ì´ˆ ê°œì„  ëª©ì ê³¼ ëŒ€ì¹˜ --> **ë¶€ì •ì  ì˜ˆë¥¼ ìƒ˜í”Œë§**í•˜ì—¬ ì‚¬ìš©
- ê¸ì •ì ì¸ ì˜ˆë¥¼ íƒ€ê¹ƒìœ¼ë¡œ í•œ ê²½ìš°ì˜ ì†ì‹¤ì„ êµ¬í•¨ + ë¶€ì •ì  ì˜ˆë¥¼ ì„ ë³„í•˜ì—¬ ì†ì‹¤ì„ êµ¬í•¨ = ìµœì¢… ì†ì‹¤
- <img src="https://imgur.com/q7v8kD4.png" alt="https://imgur.com/q7v8kD4" style="zoom:50%;" />
  - Sigmoid with Loss layerì— ê¸ì •ì  ì˜ˆëŠ” ì •ë‹µ ë ˆì´ë¸”ë¡œ 1ì„ ì…ë ¥, ë¶€ì •ì  ì˜ˆëŠ” 0ì„ ì…ë ¥í•¨ => ê° ë°ì´í„°ì˜ ì†ì‹¤ì„ ë”í•´ ìµœì¢… ì†ì‹¤ì„ ì¶œë ¥



##### 4.2.6 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì˜ ìƒ˜í”Œë§ ê¸°ë²•

- ë¶€ì •ì  ì˜ˆë¥¼ ì–´ë–»ê²Œ ìƒ˜í”Œë§í•  ê²ƒì¸ê°€?

  - ë§ë­‰ì¹˜ í†µê³„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒ˜í”Œë§ -> ë‹¨ì–´ ë¹ˆë„ ê¸°ì¤€ìœ¼ë¡œ ìƒ˜í”Œë§ --> ì¶œí˜„ íšŸìˆ˜ë¥¼ êµ¬í•´ í™•ë¥ ë¶„í¬ë¡œ ë‚˜íƒ€ë‚´ê³  ê·¸ì— ë”°ë¼ ë‹¨ì–´ë¥¼ ìƒ˜í”Œë§
  - <img src="https://imgur.com/kOSbZsI.png" alt="https://imgur.com/kOSbZsI" style="zoom:50%;" align="left"/>
  - í¬ì†Œí•œ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¼ì€ ì¤‘ìš”ë„ê°€ ë‚®ìœ¼ë¯€ë¡œ í¬ì†Œë‹¨ì–´ëŠ” íƒˆë½ë˜ëŠ” ê²ƒì´ ì¢‹ìŒ

- np.random.choice()ë¥¼ í™œìš©í•˜ì—¬ ë¬´ì‘ìœ„ ìƒ˜í”Œë§

- ```python
  # í™•ë¥ ë¶„í¬ì— ë”°ë¼ ë„¤ê±°í‹°ë¸Œìƒ˜í”Œë§ í•˜ê¸°
  np.random.choice(10)
  np.random.choice(10)
  
  words=['you', 'say', 'goodbye', 'I', 'hello', '.']
  np.random.choice(words)
  
  # 5ê°œë§Œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§(ì¤‘ë³µO)
  np.random.choice(words, size=5)
  
  # 5ê°œë§Œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§(ì¤‘ë³µX)
  np.random.choice(words, size=5, replace=False)
  
  # í™•ë¥ ë¶„í¬ì— ë”°ë¼ ìƒ˜í”Œë§
  p=[0.5, 0.1, 0.05, 0.2, 0.05, 0.1]
  np.random.choice(words, p=p)
  ```

- í™•ë¥ ë¶„í¬ì˜ ê° ìš”ì†Œì— 0.75ë¥¼ ì œê³±í•¨ìœ¼ë¡œì¨ ì¶œí˜„í™•ë¥ ì´ ë‚®ì€ ë‹¨ì–´ë¥¼ ë²„ë¦¬ì§€ ì•Šê²Œí•¨

  - ```python
    p =[0.7, 0.29, 0.01]
    new_p = np.power(p, 0.75)
    new_p /= np.sum(new_p)
    print(new_p)
    ```

- ìœ ë‹ˆê·¸ë¨ ìƒ˜í”ŒëŸ¬; í•œ ë‹¨ì–´ë¥¼ ëŒ€ìƒìœ¼ë¡œ í™•ë¥ ë¶„í¬ë¡œ ë§Œë“¦

  - ```python
    # ìœ ë‹ˆê·¸ë¨; í•˜ë‚˜ì˜ ì—°ì†ëœ ë‹¨ì–´ -> í•œ ë‹¨ì–´ë¥¼ ëŒ€ìƒìœ¼ë¡œ í™•ë¥ ë¶„í¬ë¥¼ ë§Œë“¦
    class UnigramSampler:
        def __init__(self, corpus, power, sample_size):
            # ë‹¨ì–´IDëª©ë¡(corpus), í™•ë¥ ë¶„í¬ì— ì œê³±í•  ê°’(power; default==0.75), ë¶€ì • ì˜ˆ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•  íšŸìˆ˜(sample_size)
            self.sample_size = sample_size
            self.vocab_size = None
            self.word_p = None
    
            counts = collections.Counter()
            for word_id in corpus:
                counts[word_id] += 1
    
            vocab_size = len(counts)
            self.vocab_size = vocab_size
    
            self.word_p = np.zeros(vocab_size)
            for i in range(vocab_size):
                self.word_p[i] = counts[i]
    
            self.word_p = np.power(self.word_p, power)
            self.word_p /= np.sum(self.word_p)
    
        # targetìœ¼ë¡œ ì§€ì •í•œ ë‹¨ì–´ë¥¼ ê¸ì •ì  ì˜ˆë¡œ í•´ì„í•˜ì—¬ ê·¸ ì™¸ì˜ ë‹¨ì–´IDë¥¼ ìƒ˜í”Œë§í•¨(ë¶€ì •ì ì˜ˆì‹œë¥¼ ì„ íƒ)
        def get_negative_sample(self, target):
            batch_size = target.shape[0]
    
            if not GPU:
                negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)
    
                for i in range(batch_size):
                    p = self.word_p.copy()
                    target_idx = target[i]
                    p[target_idx] = 0
                    p /= p.sum()
                    negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)
            else:
                # GPU(cupyï¼‰ë¡œ ê³„ì‚°í•  ë•ŒëŠ” ì†ë„ë¥¼ ìš°ì„ í•œë‹¤.
                # ë¶€ì •ì  ì˜ˆì— íƒ€ê¹ƒì´ í¬í•¨ë  ìˆ˜ ìˆë‹¤.
                negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),
                                                   replace=True, p=self.word_p)
    
            return negative_sample
    ```

    ```python
    # ìœ ë‹ˆê·¸ë¨ ìƒ˜í”ŒëŸ¬ ì‚¬ìš©í•˜ê¸°
    corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])
    power = 0.75
    sample_size = 2
    
    sampler = UnigramSampler(corpus, power, sample_size)
    target = np.array([1,3,0]) # ê¸ì •ì  ì˜ˆë¡œ 3ê°œì˜ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¡œ ì‚¬ìš©
    negative_sample = sampler.get_negative_sample(target)
    print(negative_sample)
    ```



##### 4.2.7 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ êµ¬í˜„

```python
# ì´ˆê¸°í™” ë©”ì†Œë“œ
class NegativeSamplingLoss:
    def __init__(self, W, corpus, power=0.75, sample_size=5):
      	#ì¶œë ¥ì¸¡ ê°€ì¤‘ì¹˜ W, ë§ë­‰ì¹˜(ë‹¨ì–´id list) corpus, 
        #í™•ë¥ ë¶„í¬ì— ì œê³±í•  ê°’ power, ë¶€ì •ì  ì˜ˆ ìƒ˜í”Œë§ íšŸìˆ˜ sample 
        self.sample_size = sample_size # ë¶€ì •ì  ìƒ˜í”Œë§ íšŸìˆ˜ë¥¼ ì¸ìŠ¤í„´ìŠ¤ë³€ìˆ˜ì— ì €ì¥
        self.sampler = UnigramSampler(corpus, power, sample_size)
        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)] #sigmoid with loss ê³„ì¸µì„ ì €ì¥ + 1(0ë²ˆì§¸ ê³„ì¸µ; ê¸ì •ì ì˜ˆ)
        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)] # embedding dot ê³„ì¸µì„ ì €ì¥ + 1(0ë²ˆì§¸ ê³„ì¸µ; ê¸ì •ì ì˜ˆ)
				
        # ê° ê³„ì¸µì—ì„œ ì‚¬ìš©í•  ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì™€ ê¸°ìš¸ê¸°ë¥¼ ë°°ì—´ë¡œ ì €ì¥
        self.params, self.grads = [], []
        for layer in self.embed_dot_layers:
            self.params += layer.params
            self.grads += layer.grads

    def forward(self, h, target):
      	# ì€ë‹‰ì¸µ ë‰´ëŸ° h, ê¸ì •ì  ì˜ˆ target
        batch_size = target.shape[0]
        
        #ë¶€ì •ì  ì˜ˆë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ì €ì¥
        negative_sample = self.sampler.get_negative_sample(target)

        # ê¸ì •ì  ì˜ˆ ìˆœì „íŒŒ
        score = self.embed_dot_layers[0].forward(h, target) #Embedding Dot ê³„ì¸µì˜ forward score
        correct_label = np.ones(batch_size, dtype=np.int32)
        loss = self.loss_layers[0].forward(score, correct_label) # sigmoid with loss ê³„ì¸µìœ¼ë¡œ í˜ë ¤ lossë¥¼ êµ¬í•¨

        # ë¶€ì •ì  ì˜ˆ ìˆœì „íŒŒ
        negative_label = np.zeros(batch_size, dtype=np.int32)
        for i in range(self.sample_size):
            negative_target = negative_sample[:, i]
            score = self.embed_dot_layers[1 + i].forward(h, negative_target)
            loss += self.loss_layers[1 + i].forward(score, negative_label)

        return loss

    def backward(self, dout=1):
        dh = 0
        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):
            dscore = l0.backward(dout)
            dh += l1.backward(dscore)

        return dh
```



### 4.3 ê°œì„ íŒ word2vec í•™ìŠµ

- PTB ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ì„œ í•™ìŠµí•˜ê³  ì‹¤ìš©ì ì¸ ë‹¨ì–´ ë¶„ì‚°í‘œí˜„ ì–»ê¸°

##### 4.3.1 CBOW ëª¨ë¸ êµ¬í˜„

- í´ë˜ìŠ¤ ì¶œë ¥ì¸¡ ê°€ì¤‘ì¹˜ëŠ” ì…ë ¥ì¸¡ ê°€ì¤‘ì¹˜ì™€ ê°™ì€ í˜•ìƒ -> ë‹¨ì–´ ë²¡í„°ê°€ í–‰ ë°©í–¥ì— ë°°ì¹˜ --> Embedding ê³„ì¸µ ë•Œë¬¸

- ```python
  # CBOW í‘œí˜„
  # coding: utf-8
  import sys
  from common.np import *  # import numpy as np
  # from common.layers import Embedding
  
  class CBOW:
      def __init__(self, vocab_size, hidden_size, window_size, corpus):
        	# vocab_size;ì–´íœ˜ìˆ˜, hidden_size; ì€ë‹‰ì¸µì˜ ë‰´ëŸ° ìˆ˜,
          # corpus; ë‹¨ì–´ ID ëª©ë¡, window_size; ë§¥ë½ì˜ í¬ê¸°(íƒ€ê¹ƒì„ ì¤‘ì‹¬ìœ¼ë¡œ ì¢Œìš°ë¡œ +@)
          V, H = vocab_size, hidden_size
  
          # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
          W_in = 0.01 * np.random.randn(V, H).astype('f')
          W_out = 0.01 * np.random.randn(V, H).astype('f')
  
          # ê³„ì¸µ ìƒì„±
          self.in_layers = []
          for i in range(2 * window_size):
              layer = Embedding(W_in)  # Embedding ê³„ì¸µ ì‚¬ìš©
              self.in_layers.append(layer)
          self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)
  
          # ëª¨ë“  ê°€ì¤‘ì¹˜ì™€ ê¸°ìš¸ê¸°ë¥¼ ë°°ì—´ì— ëª¨ìŒ
          layers = self.in_layers + [self.ns_loss]
          self.params, self.grads = [], []
          for layer in layers:
              self.params += layer.params
              self.grads += layer.grads
  
          # ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ë‹¨ì–´ì˜ ë¶„ì‚°í‘œí˜„ì„ ì €ì¥
          self.word_vecs = W_in
  
      def forward(self, contexts, target):
        	# ë§¥ë½ê³¼ íƒ€ê¹ƒì„ ë‹¨ì–´IDë¡œ ë°›ìŒ(SimpleCBOWëŠ” í¬ì†Œë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©)
          h = 0
          for i, layer in enumerate(self.in_layers):
              h += layer.forward(contexts[:, i])
          h *= 1 / len(self.in_layers)
          loss = self.ns_loss.forward(h, target)
          return loss
  
      def backward(self, dout=1):
          dout = self.ns_loss.backward(dout)
          dout *= 1 / len(self.in_layers)
          for layer in self.in_layers:
              layer.backward(dout)
          return None
  ```



##### 4.3.2 CBOW ëª¨ë¸ í•™ìŠµ ì½”ë“œ

- ```python
  # coding: utf-8
  import sys
  # sys.path.append('..')
  import numpy as np
  from common import config
  # GPUì—ì„œ ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”(CuPy í•„ìš”).
  # ===============================================
  # config.GPU = True
  # ===============================================
  import pickle
  # from common.trainer import Trainer
  # from common.optimizer import Adam
  # from cbow import CBOW
  # from skip_gram import SkipGram
  from common.util import to_cpu, to_gpu
  from dataset import ptb
  
  
  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
  window_size = 5	#ìœˆë„ìš° í¬ê¸°(ë§¥ë½ í¬ê¸°)
  hidden_size = 100 #ì€ë‹‰ì¸µ ë‰´ëŸ° ìˆ˜
  batch_size = 100
  max_epoch = 10
  
  # ë°ì´í„° ì½ê¸°
  corpus, word_to_id, id_to_word = ptb.load_data('train')
  vocab_size = len(word_to_id)
  
  contexts, target = create_contexts_target(corpus, window_size)
  if config.GPU:
      contexts, target = to_gpu(contexts), to_gpu(target)
  
  # ëª¨ë¸ ë“± ìƒì„±
  model = CBOW(vocab_size, hidden_size, window_size, corpus)
  # model = SkipGram(vocab_size, hidden_size, window_size, corpus)
  optimizer = Adam()
  trainer = Trainer(model, optimizer)
  
  # í•™ìŠµ ì‹œì‘
  trainer.fit(contexts, target, max_epoch, batch_size)
  trainer.plot()
  
  # ë‚˜ì¤‘ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•„ìš”í•œ ë°ì´í„° ì €ì¥
  word_vecs = model.word_vecs
  if config.GPU:
      word_vecs = to_cpu(word_vecs)
  params = {}
  params['word_vecs'] = word_vecs.astype(np.float16)
  params['word_to_id'] = word_to_id
  params['id_to_word'] = id_to_word
  pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'
  with open(pkl_file, 'wb') as f:
      pickle.dump(params, f, -1)
  
  ```

  

##### 4.3.3  CBOW ëª¨ë¸ í‰ê°€

- most_similar() ë©”ì†Œë“œë¡œ ê±°ë¦¬ê°€ ê°€ê¹Œìš´ ë‹¨ì–´ë¥¼ ë½‘ìŒ

  - ```python
    # coding: utf-8
    import sys
    sys.path.append('..')
    from common.util import most_similar, analogy
    import pickle
    
    
    pkl_file = 'cbow_params.pkl'
    # pkl_file = 'skipgram_params.pkl'
    
    with open(pkl_file, 'rb') as f:
        params = pickle.load(f)
        word_vecs = params['word_vecs']
        word_to_id = params['word_to_id']
        id_to_word = params['id_to_word']
    
    # ê°€ì¥ ë¹„ìŠ·í•œ(most similar) ë‹¨ì–´ ë½‘ê¸°
    querys = ['you', 'year', 'car', 'toyota']
    for query in querys:
        most_similar(query, word_to_id, id_to_word, word_vecs, top=5)
    
    # ìœ ì¶”(analogy) ì‘ì—…
    print('-'*50)
    analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)
    analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)
    analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)
    analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)
    ```

- ë¹„ìŠ·í•œ ë‹¨ì–´ë¥¼ ëª¨ìœ¼ê³  ë³µì¡í•œ íŒ¨í„´ì„ íŒŒì•…

- word2vec ë‹¨ì–´ë¶„ì‚°í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ìœ ì¶”ë¬¸ì œë¥¼ ë²¡í„°ì˜ ë§ì…ˆ, ëº„ì…ˆìœ¼ë¡œ í’€ ìˆ˜ ìˆìŒ

  - <img src="https://imgur.com/mfWgX1h.png" alt="https://imgur.com/mfWgX1h" style="zoom:50%;" />
    - ë‹¨ì–´ë²¡í„° ê³µê°„ì—ì„œ íŠ¹ì • ë‹¨ì–´ì™€ ê°€ëŠ¥í•œ ê°€ê¹Œì›Œì§€ëŠ” ë‹¨ì–´ë¥¼ ì°¾ì•„ ìœ ì¶” ë¬¸ì œ í•´ê²°

- ì‹œì œíŒ¨í„´, ë‹¨ìˆ˜/ë³µìˆ˜, ë¹„êµê¸‰ ì„±ì§ˆ ë“±ì´ ì¸ì½”ë”© ë˜ì–´ ìˆìŒì„ í™•ì¸ -> ë¬¸ë²•ì  íŒ¨í„´ë„ íŒŒì•… ê°€ëŠ¥



### 4.4 word2vec ë‚¨ì€ ì£¼ì œ

4.4.1 word2vecì„ ì‚¬ìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì˜ˆ

- ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ --> ë‹¤ë¥¸ ëª¨ë¸ì˜ inputìœ¼ë¡œ í™œìš© -> **ì „ì´í•™ìŠµ**
  - í•™ìŠµì„ ë¯¸ë¦¬ ëë‚¸ ë‹¨ì–´ ë¶„ì‚° í‘œí˜„ì„ ì´ìš©í•˜ë©´ NLP ì‘ì—…ì´ ëŒ€ì²´ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§
- ë¬¸ì¥, ë‹¨ì–´ë¥¼ ê³ ì •ê¸¸ì´ ë²¡í„°ë¡œ ë³€í™˜ -> ìì—°ì–´ì²˜ë¦¬ì— ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì„ ì ìš©í•  ìˆ˜ ìˆìŒ
  - Bag-of-words; ë‹¨ì–´ ìˆœì„œë¥¼ ê³ ë ¤ì¹˜ ì•Šê³  ë¬¸ì¥ì˜ ê° ë‹¨ì–´ë¥¼ ë¶„ì‚°í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•´ í•©ì„ êµ¬í•˜ëŠ” ë°©ë²•
  - ìˆœí™˜ì‹ ê²½ë§(RNN) í™œìš©
  - <img src="https://imgur.com/iFYqt28.png" alt="https://imgur.com/iFYqt28" style="zoom:50%;" />



##### 4.4.2 ë‹¨ì–´ ë²¡í„° í‰ê°€ ë°©ë²•

- ë‹¨ì–´ ë¶„ì‚° í‘œí˜„ í‰ê°€ëŠ” ì‹¤ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ ë¶„ë¦¬í•´ í‰ê°€
- ë‹¨ì–´ì˜ ìœ ì‚¬ì„±, ìœ ì¶”ë¬¸ì œë¥¼ í™œìš©í•˜ì—¬ í‰ê°€
- ìœ ì‚¬ì„± í‰ê°€; ì‚¬ëŒì´ ì‘ì„±í•œ ë‹¨ì–´ ìœ ì‚¬ë„ë¥¼ ê²€ì¦ ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•´ í‰ê°€
  - ì‚¬ëŒì´ ë¶€ì—¬í•œ ì ìˆ˜ì™€ word2vecì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ë¹„êµí•´ ìƒê´€ì„±ì„ ë´„
- ìœ ì¶”ë¬¸ì œ í™œìš© í‰ê°€; ìœ ì¶”ë¬¸ì œë¥¼ ì¶œì œí•´ ì •ë‹µë¥ ë¡œ ì¸¡ì •
  - <img src="https://imgur.com/PjGhUZa.png" alt="https://imgur.com/PjGhUZa" style="zoom:50%;" />
  - ë‹¨ì–´ì˜ ì˜ë¯¸, ë¬¸ë²•ì  ë¬¸ì œë¥¼ ì–´ëŠì •ë„ ì´í•´í•˜ëŠ”ì§€ ì¸¡ì • ê°€ëŠ¥



### 4.5 ì •ë¦¬

- Embedding ê³„ì¸µì„ êµ¬í˜„í•˜ê³  ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ê¸°ë²•ì„ ë„ì…í•´ ì¼ë¶€ë§Œ ì²˜ë¦¬í•˜ì—¬ ê³„ì‚°ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰
- word2vecìœ¼ë¡œ ì–»ì€ ë‹¨ì–´ ë¶„ì‚°í‘œí˜„ì€ ë‹¤ì–‘í•œ ìì—°ì–´ì²˜ë¦¬ ì‘ì—…ì— í™œìš©

<img src="https://imgur.com/82Zj3Qy.png" alt="https://imgur.com/82Zj3Qy" style="zoom:50%;" />

 