---
title: "데이터야놀자 컨퍼런스 정리"
classes: wide
categories:
  - Conference/Seminar
tags:
  - AI
  - NLP/NLU
  - DataScience
author_profile : true
use_math : true
last_modified_at: 2019-09-05
---

|               |                                                              |                                                              |
| ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|               | 데[da]월관                                                   | 호텔 데[da]루나                                              |
| 10:00 ~ 10:10 | 등록                                                         |                                                              |
| 10:10 ~ 10:40 | **이거 좋아하니? Serverless에서 유저 컨텐츠 추천 서비스 in 빙글** | **GAN을 활용한, 내 손글씨를 따라쓰는 인공지능**              |
| 10:40 ~ 10:50 | BREAK                                                        |                                                              |
| 10:50 ~ 11:20 | **나의 리터러시≠너의 리터러시 : 잘 전달하기 위해 데이터 시각화할 때 고려하는 것들** | 와디즈 앵콜 가죽자켓 예시로 보는 크라우드펀딩 기업 가치평가  |
| 11:20 ~ 11:30 | BREAK                                                        |                                                              |
| 11:30 ~ 12:00 | **자연어 처리 모델의 성능을 높이는 비결 - 임베딩**           | 한글 검색, 고민해야 할게 많아요                              |
| 12:00 ~ 12:45 | LUNCH                                                        |                                                              |
| 12:45 ~ 13:00 | <IGNIGHT>  식습관, 스몰 데이터 분석을 통한 장트러블 극복기   |                                                              |
| 13:00 ~ 13:40 | **IT 서비스 회사에서 Platform Player로 살아남기 :Public Cloud 분석 플랫폼 AccuInsight+ 개발기** | **카트라이더 TMI 포스트 모템**                               |
| 13:40 ~ 13:50 | BREAK                                                        |                                                              |
| 13:50 ~ 14:20 | **웹으로 표현하는 데이터 시각화와 스토리텔링**               | **헬스케어 데이터 실습**                                     |
| 14:20 ~ 14:30 | BREAK                                                        |                                                              |
| 14:30 ~ 15:10 | **타다(TADA) 서비스의 데이터 웨어하우스 : 태초부터 현재까지** | **데이터 분석팀과 이스포츠 선수단의 신뢰쌓기 프로세스**      |
| 15:10 ~ 15:25 | 함께 성장하는 커뮤니티 리더십 (Snack Time)                   |                                                              |
| 15:25 ~ 15:40 | <IGNIGHT>  혹시 당신도 거북이세요? 비전공자, 거북목 프로젝트로 인공지능에 가까워지기 |                                                              |
| 15:40 ~ 16:20 | **분석과 커뮤니케이션의 시행착오를 줄이는 데이터 문화 만들기 - 소셜 데이팅 앱 '글램'의 데이터분석 가이드라인** | **심슨의 역설**                                              |
| 16:20 ~ 16:30 | BREAK                                                        |                                                              |
| 16:30 ~ 17:10 | **데이터 + 야놀자 = 야놀자에서 데이터를 더하는 법**          | **온라인 뉴스 댓글 생태계를 흐리는 어뷰저 분석기**           |
| 17:10 ~ 17:25 | <IGNIGHT>  문화생활 좋아하세요? 데이터로 분석한 프로 덕질러의 텅장 일대기 |                                                              |
| 17:25 ~ 18:00 | **모빌리티 데이터팀 신입 분석가의 1년 회고**                 | **데이터 교육 프로덕트 매니저의 고민과 실패 그리고 성장 이야기** |


#### 1. 나의 리터러시≠너의 리터러시 : 잘 전달하기 위해 데이터 시각화할 때 고려하는 것들

---

1. 개요

>  연사 소개; kbs 디지털뉴스 데이터저널리즘팀 데이터 시각화 업무를 맡고 있음 (디자인)

- 데이터분석가와 디자니어, 기자 간의 데이터 리터러시가 각기 달라 이를 합치 하기가 어려웠음
- 같은 데이터이지만 사람마다 다양한 관점에서 바라보고, 이에 따라 시각화 자료도 달라짐



2. 사례; 야구 관련 기사 작성시 필요한 데이터 시각화
   1. 누적된 데이터를 주면 다양한 모양으로 그래프를 스케치
   2. 기자, 데이터분석가, 개발자, 디자이너 4자간 의견 교류
      - 분석가가 R로 시각화한 데이터를 주기도 함; R2Ilerstrator 등을 사용해서 이미지로 만듦
3. QNA
   - 커뮤니케이션 이슈가 있었는지?
     - 새로운 시도를 해볼 때 이슈가 발생하긴 하지만 여러 스케치를 가져와서 내부테스트를 진행
     - 동료검토 진행

#### 2.자연어 처리 모델의 성능을 높이는 비결 - 임베딩

---

![img](https://datayanolja.github.io/images/presenters/leekichang.png)발표자 | 이기창

NAVER Chatbot Model 팀에서 대화 모델을 개발합니다. 자연어 처리 [블로그](https://ratsgo.github.io/)를 운영 중입니다. 딥러닝과 자연어 처리의 가능성을 믿습니다.



1. 임베딩?

   - 단어를 벡터로 바꾼 것 또는 그 과정
   - 단어를 벡터스페이스에 끼워넣는다
   - 단어나 문장의 빈도를 그대로 임베딩으로 사용하는 방법
     - 토큰화한 문장의 열을 문서벡터로, 행을 단어벡터로 사용하는 방법
   - Word2vec 임베딩

2. 임베딩으로 할 수 있는 것?

   - 컴퓨터가 이해할 수 있게 만들어준다! -> 컴퓨터 연산 가능

   - 코사인 유사도를 뽑아서 유사단어, 관련단어를 도출할 수 있음

   - 시각화

     - t-sne로 100차원의 w2v 결과를 저차원으로 축소하여 시각화할 수 있음

   - 벡터 연산(유추 평가)

     - word analogy task
     - 아들 - 딸 + 소녀 = 소년

   - 전이학습

     - 다른 딥러닝 모델의 입력값으로 사용

       > 예) 영화 평가의 감정분석시 입력값으로 잘 학습된 단어임베딩을 사용

     - 인간이 과거의 학습내용을 생각해서 새로운 개념을 받아들이듯이, 기존 코퍼스를 학습한 뒤 새로운 것을 받아들이는 것이 머신도 학습결과가 좋음

     - 좋은 임베딩 모델을 사용하면 딥러닝 모델도 좋은 결과를 얻을 수 있다!

3. 단어 임베딩으로 문서분류하기

   1. 단어 임베딩 구축; FastText 사용
      - fasttext를 사용하여 임베딩
      - 문서벡터 == 단어 벡터의 합 ~ 단어벡터의 평균 ~ 문서 벡터의 중심으로 여김
   2. 네이버 영화리뷰 말뭉치를 활용하여 실험
      - 학습데이터 댓글을 문서벡터로 변환(단어 임베딩의 합)
      - 학습 5만 / 테스트 1만건으로 이진분류 모델을 만듦
      - 워드임베딩 -> 워드임베딩의 합 -> 코사인 유사도를 도출 하는 단순한 작업만 했음에도 불구하고 72퍼센트정도의 높은 성능을 보여줌
      - => 임베딩 품질이 좋으면 자연어 처리 문제의 많은 부분을 해결할수 있다!
      - ELMO 80점대, BERT 90점대의 성능을 보여줌
   3. https://github.com/ratsgo/embedding

4. 임베딩이 어떻게 의미를 가지는가?

   1. 말뭉치의 통계적패턴을 가지고있음

      - 빈도를 카운트하여 그대로 또는 약간의 처리를 해서 임베딩으로 사용
        - 작성자의 의도는 단어 사용 패턴에 있다고 가정하고 만든것

   2. 단어가 어떤 순서로 나타나는지 살펴봄

      - 단방향; ELMo, GPT
      - 양방향; BERT - SOTA 모델

   3. 단어가 어떤 단어와 주로 같이 나타나는지? -> 분포

      - => 문맥에 의미가 내포되어 있다는 전제!

      - word2vec , FastText, GloVe
      - 목적함수들은 대부분 이런 방식을 사용하고 있음
      - 자세한 내용은 구글에 기법을 서치해볼 것

5. 문장 수준 임베딩

   1. 현재 핫한 기술들(Transformer, ELMo, BERT)

      - 양방향 LSTM, 미리 코퍼스 학습되어있는 모델을 사용, 파인튜닝하는 방식으로 학습
      - 네이버에선 다 쓰고 있고 전체 도메인에서 성능향상이 있었음

   2. 동음이의어를 분간할 수 있음 -> 문장의 문맥적 의미를 벡터화 할 수 있음

      - 의미 차이가 임베딩에 드러나도록 계산하는 모델들

   3. ELMo

      - docker image를 만들어 두었고 이를 활용해서 실습
      - Sentmodel.sh 에서 pretrained-elmo에서 데이터 패스만 바꿔서 본인이 가지고 있는 데이터를 활용할 수 있음

   4. BERT

      - 위와 상동

      - 8개의 GPU로 2주정도 학습해야하기 때문에 학습된 버트 모델을 사용할 것

6. 임베딩 활용

   1. 전이학습(transfer learning)

      - 다른 네트워크의 입력값으로 사용하여 자연어 처리 성능을 높일 수 있음
      - 레고 블럭 쌓듯이 네트워크를 nees에 맞게 쌓아서 다양하게 활용 가능

      - 한국어 임베딩 책에서 구현 할 수 있음

7. QNA

   - 자연어 질의가 들어오면 자연어가 답변하는 시스템을 개발중이며, FAQ를 만들고 있음
     - 답변해야하는 intent의 그룹을 지어서 이 그룹클래스를 맞추는 형태로 개발중임
   - 평가는 FAQ이기 때문에 학습과 테스트를 분리할 수 없음
     - 서비스할 때에는 구분하지 않고 통으로 넣어서 학습시키고 있음
     - 사람이 평가하고 있음; 내부 평가
   - GPU는 V100 사용, 8개 사용
   - 학습데이터는 10만건 중 1만건을 제외하고 9만건을 학습, 최적값을 찾았다면 서비스에 올리고 이때는 1만건 중 100건은 early stopping으로 사용하고 나머지는 학습에 추가
   - unknown token을 해결하는 방법?
     - UNK가 있어도 벡터가 0으로 떨어지지는 않음
     - 제대로 표현하고 싶다면 말뭉치를 새롭게 학습시키는 방법밖에 없음
     - 엘모는 무조건 임베딩으로 나오기 때문에 UNK이없음
     - 버트는 보캡이 바뀌면 처음부터 프리트레인 해야하지만 애초에 UNK가 잘 안나오는 모델이기 때문에 크게 고려하지 않고 있음
     - 



#### 3. 카트라이더 TMI 포스트 모템

---

![img](https://datayanolja.github.io/images/presenters/hwangjunsik.png)발표자 | 황준식

넥슨에서 머신러닝을 이용해 더 나은 게임 개발 / 운영 / 사업에 기여하려고 노력하고 있습니다. 개인 블로그 [jsideas.net](https://jsideas.net/)을 운영하고 있습니다.

https://tmi.nexon.com/kart

1. 데이터로 재미를

   1. 넥슨에서의 데이터 활용?
      - 데이터로 대규모 작업장을 제재하거나 유저 이탈 예측, 아이템 추천 등의 task를 수행
      - 빠른/자동 의사결정/사람이 찾을 수 없는 특별한 패턴을 찾음
      - 게임의 난이도를 데이터 분석을 통해서 도출해서 더 재밌는 게임으로 만들기 위함
   2. 데이터 마이닝
      - 데이터에 감춰진 히든 패턴을 찾아 의사결정에 반영하는 과정

2. 서비스 개발과정

   - OP.GG에서 데이터를 유/무료 API로 제공하고 있음

     - LoL의 경우에는 해당기술을 접목해서 나에게 유용한 챔피언을 추천하는 시스템을 도입하였음

   - 재밌는 로그, 실시간으로 로그가 떨어져야 하는데 실시간으로 로그를 남기는 것은 게임의 성능에도 큰 영향을 미치기 때문에 어려웠음

     - 일 배치로 올리는 것과 실시간으로 올리는 것이 성능에 큰 차이가 있음
     - 업데이트 연동 등 안정성의 문제
     - 실시간 파이프라인이 없어 별도의 api를 만들어야함

   - make sense하느냐?

     - 높은 불확실성과 긴 파이프라인에 대한 부담
     - 일단 빠르게 검증해서 시장에 내놓자!

   - 짧은 단위로 이터레이션을 돌리는 방법으로 진행

   - PoC

     - 사내 유저들을 모아서 유저로써 보고 싶어하는 정보들을 유추

     - 데이터 프로토타이핑; x, y좌표를 matplotlib으로 그려보니 실제 트랙이 그려졌음
     - 사고다발구역 도출
     - 고수들의 주행경로 분석 -> 고수와 하수의 드리프트 궤적 비교

   - 오픈 당시에는 재방문유저가 적어 힘들었으나, 유저 전적 보기 서비스를 도입하면서 유저 추이가 급상승

   - +@ 모바일에서 전적을 조회할 수 있도록 하여 더욱 증가

   - 유저들이 좋아하고, 관심있어하는 수치를 분석하여 결과를 도출(ex. 톡톡이, 감속, 최고속도 등)

   - 분석가로써 직접 게이머에게 접근할 수 있는 채널이 만들어진 것!

3. 느낀점과 배운점

   1. 성공한 업데이트

      - 개인전적조회

   2. 실패한 업데이트

      - 카트 성능 비교하기 -> 실제로는 큰 영향이 없어 유저들의 관심이 적었음

   3. 아쉬움

      - 주행경로를 시각화 하려면 유저의 데이터를 모두 저장해야하는데 리소스의 부족으로 할 수 없었던 점
      - 채널파워의 중요성; tmi서비스에 대한 노출이 중요
        - 카트라이더 종료를 했을 때 tmi로 연결 --> 30~40%가 이 때 유입
        - 유튜버 크리에이터
        - Maketing Intenligence

   4. OpenAPI 준비중

      

#### 4. 헬스케어 데이터 실습

---

![img](https://datayanolja.github.io/images/presenters/hongwonjun.png)발표자 | 홍원준

헬스케어 데이터 분석 전문 스타트업인 라인웍스에서 5년째 데이터를 분석하고 있습니다. 데이터가 가장 가치가 높은 자원이 될 것이라 믿고 있습니다. [블로그](https://hongwonjun.github.io/)에 가끔 글을 적고 있습니다.

https://linewalks.com/blog

1. 개요

   - 현재 데이터 관련 각광받는 헬스케어 사업
   - 구글, 아마존에서도 병원 데이터를 일반 유저들이 직접 활용할 수 있도록 도와주고 있음
   - sns와 웨어러블 디바이스의 데이터들도 모두 오픈하여 의료데이터로써 사용할 수 있음
   - 이용할 데이터 및 실습 방법 안내

2. 데이터; MIMIC-III

   - EMR데이터
     - 병원에서 쓰이는 시스템에서 기록되는 데이터를 통칭함 -> 의료데이터
     - 공개된 데이터여서 분석코드, 논문 및 블로그 포스트들도 많음
   - https://mimic.physionet.org 에서 다운로드
     - 간단한 교육을 거쳐서 워킹데이로 3일정도 뒤에 다운로드 가능
     - docker에 올려서 사용
   - 자연어처리 / 이벤트 예측 / 유사환자 탐색(클러스터링)

3. 프로세스 수립

   1. 환자군 정의(Cohort)

      - 전체 데이터 중 신생아 데이터가 꽤 있어서 성능이 잘 나오지 않을 수 있음 -> 과감히 제거

   2. 타겟 정의 및 라벨링

      - 어떤 이벤트를 예측할 것인가?

      - 사망 / 특정 조건의 조합(응급입원 후 심장수술 여부) / 퇴원 후 30일 내 재입원 이슈

        > 퇴원 후 30일 내 재입원인 경우 sorting 후 재입원 여부에 따라 라벨링
        >
        > 사망이벤트의 경우 사망 후 재방문이 있었음 -> 장기이식이라는 특이사항!

      - 환자별 정렬 -> 한명의 환자 당 하나의 row로 구성, 재입원 또는 사망 라벨을 붙임

      - 방문별 정렬 -> 방문했을 때 어떤 이벤트가 있었는지 하나의 row로 나열

   3. 사용할 변수 선택

      - 도메인지식이 있으면 유리한 태스크이지만 삽질의 연속...
      - 예측하려는 정보(Y)와 같은 정보를 변수(X)로 사용했을 때 성능이 과하게 좋게 나올 수 있음
        -  너무 당연한 이야기를 예측하는것(ex. y=고혈압발생확률, X=혈압)
      - 임베딩을 통해 코드로 변환(Skip-gram)

   4. 모델 선택

      - 모델끼리의 성능 차이가 거의 없기 때문에 Linear Regression부터 시작해도 무방함
      - 만약 어렵다면 LightGBM을 추천함

   5. 처음으로

      - 모델 성능을 보고 처음으로 돌아가 어떤 변수를 설정할지, 레이어를 고칠지 고민

      - 퇴원 후 30일 이내 재입원의 문제를 풀었을 때

        - >  AUROC 65~70이면 괜찮은 성능
          >
          > 60이하면 잘못하고있음
          >
          > 80 이상이어도 잘못... -> 과하게 성능이 좋게 나온 것

4. 정리

   - 헬스케어데이터도 일반적인 데이터와 똑같으니 꼭 도전해보길!

     

#### 5. 타다(TADA) 서비스의 데이터 웨어하우스 : 태초부터 현재까지

---

![img](https://datayanolja.github.io/images/presenters/leechanghyun.png)발표자 | 이창현

VCNC 에서 데이터 엔지니어 (+ SQL 전도사) 로 활동하고 있습니다. 궁금한 것들을 찾아서 이해하고, [개발 블로그](https://chang12.github.io/)에 적는걸 좋아합니다.

1. 개요
   - vcnc? between, 타다 개발
   - 타다; 모빌리티 플랫폼
   - 런칭 1년간의 데이터웨어하우스의 변천사
2. 사용한 툴, 기술 소개
   1. Aurora MySQL
      - RDBMS여서 SQL가능 (between은 HBase(noSQL))
   2. Holistics
      - 유료 BI서비스로, 다양한 데이터소스를 연결
      - 다이나믹 필터기능; SQL을 적고 blank를 두면 유저가 원하는 값으로 포맷팅해줌
        - SQL을 보지않고도 대시보드를 사용하는 유저들이 조회를 원하는 값으로할 수 있음
      - UI설정만으로 쿼리 결과를 다양하게 시각화할 수 있음
3. 데이터를 한 곳으로
   - RDBMS들을 띄움 -> apache spark -> AWS RDS
   - spark? 다양한 데이터소스를 똑같은 형태로 읽고 쓸 수 있도록 해줌
     - 읽을 데이터 종류와 정보만 알려주면 하나의 데이터프레임으로 만들어서 쓸 수 있게해줌
4. MySQL 8 / PostgreSQL 10
   - 데이터 마이그레이션 경험
   - MySQL8의 단점; text타입으로 partition by 했을 때 데이터가 달라지는 경우가 있었음 (char로 바꾸면 해결)
   - PostgreSQL 10은 Array 등을 제공하고 더욱 복잡한 자료구조를 지원하고 있었음
   - 



####6. 분석과 커뮤니케이션의 시행착오를 줄이는 데이터 문화 만들기 - 소셜 데이팅 앱 '글램'의 데이터분석 가이드라인

---

![img](https://datayanolja.github.io/images/presenters/leekyungjin.png)발표자 | 이경진

관계와 사랑의 글램 데이터를 분석하고 알고리즘을 설계하는, 큐피스트 데이터분석가 Kaya입니다. 대학에서 통계학을, 대학원에서 경영학을 공부했어요. "좋은 분석가"를 꿈꿉니다.

1. 분석의 관점
   1. 분석의 관점에서 본 우리 서비스
      - 문화, 성별에 따라 앱을 사용하는 패턴/유형이 달라진다는 점
      - 건강한 유저풀 관리를 위해 까다로운 가입절차를 구성
      - 불량 유저 방지
   2. 분석의 관점에서 본 우리 회사
      - 빠른 의사결정
      - 데이터기반 의사결정이 일상화되어있음; 각 부서들이 데이터에 대한 적극적인 needs를 가지고있음
      - BUT, 분석가를 제외한 나머지 구성원들은 데이터 접근에 한계가 많음; 더욱 섬세한 소통이 필요

2. 분석의 지향점과 덜지향점?(우선순위 가치)

   1. 빠르게 분석해야한다; 빠른 분석으로 빠른 의사결정을 지원
      - 3일이상 기다리게 하지 않는다
      - => 넓고 얕은 분석, 탐색적 데이터 탐험은 최소한으로
   2. 분명하게 분석해야한다; 분명한 견해를 제시하여 결정 과정을 가속화
      - 데이를 통해 발견한 내용에 대해서 분명하게 정의하고, 그래서 각 팀은 무엇을 해야하는지 구체적으로 제시할 필요가 있음
      - 데이터 접근성이 2명의 분석가로 한정적이기 때문에 빠르게 목적에 도달하기 위해 분석의 범주를 넓게 가져가고 있음
   3. 쉽게 분석해야한다.; 쉬운 설명
      - 누구든지 3초만에 이해할 수 있도록! -> 커뮤니케이션의 문제
      - 통계적으로 얼마나 의미있는지, 얼마나 뛰어난지 이야기하기 보다는 데이터 리터러시가 가장 낮은 사람에 초점을 맞춰 그가 이해할 수 있도록 함
   4. 정확하게 분석해야한다; 분석가에 대한 의존도가 높으므로 무결점의 정확성을 지향
      - 실수를 하지 않는다... 무결점의 정확성
   5. "덜지향점": 깊고 상세한 분석, 가독성과 재사용성이 뛰어난 코드

3. 분석을 위한 실무 가이드라인을 만들자

   1. 회사의 특성과 서비스의 특성을 결합해 분석의 지향점으로 갈 수 있게 하는 프로세스를 구성
   2. 데이터분석업무의 주된 흐름
      - 기획; 빠른 분석을 도와주는 기획가이드
      - 분석; 실수를 방지하는 분석 가이드
      - 커뮤니케이션; 직관성을 극대화 하는 커뮤니케이션 가이드

4. 실무 가이드라인!

   1. 빠른 항해를 도와주는 기획 가이드

      1. 지양해야할 것
         1. 의미없이 데이터분석부터 하는 것
         2. DAU, 재방문률과 같은 후행지표를 무작정 접근하는 것

      2. 지향해야할 것

         - 촘촘한 가설 수립하기; 예상되는 결론을 글로 풀어쓸 수 있을 정도로 기획의 높은 완결성 추구

           >  좋은 예와 나쁜 예에 대하여 가설을 계속해서 이어나감
           >
           > 이를 통해 분석을 할 때 무엇을 중점적으로 봐야하는지 확실하게 알 수 있음
           >
           > 가설을 작성하면서 있던 빈칸만 채우면 끝난다! 라는 개념으로 접근해야함
           >
           > 내가 예상했던 것과 어느 부분이 달라지는지 그 차이에 집중하는 방법으로 접근할 수 도 있음

         - 분석의 decision tree 만들기

           > 결과가 예상과 다른 경우 방향 전환 계획과 그에 따른 결론에 대한 경우의 수 정리
           >
           > 구체적으로 시나리오를 구축해서 시행착오를 줄일 수 있게 함

         - 데이터를 보지 않은 상태로 팀 리뷰를 하며 기획을 보강

           > 분석의 논리적 흐름을 검토하는 데 용이함

   2. 실수를 방지하는 분석 가이드

      1. 분석 기간과 단위 설정 지침

         - 서비스 히스토리를 반영한 분석 기간 필터 기준을 설정(타임라인 설정)

         - 통일된 기간 단위를 선정해서 혼선을 방지함

           > 각 언어의 함수에 따라 결과가 달라질 수 있기 때문에 아예 기준을 확정해놓음

      2. 데이터 추출 지침

         - 쿼리, 전처리, 네이밍에서의 컨벤션

           > 유저정보 추출시에 1:1 구조인 프로필 데이터는 하나의 데이터셋으로
           >
           > 혼선을 막기 위해 유저 필터 기준을 오브젝트 명에 명시
           >
           > 시계열데이터 추출시에는 네이밍을 통일하고 timezone 누락방지를 위한 구체적 지침 정의

      3. 유저 필터 및 분류 지침

         - 서비스 특성을 고려하여 유저 분류를 일관성 있게 적용함

           > 유저 필터 기준과 GROUP BY 기준을 정의함

   3. 직관성을 극대화하는 커뮤니케이션 가이드

      1. 용어 가이드

         - 청자의 혼동을 막기 위해 정확한 용어를 사용함

           > 용어 사전 정의하여 일관성 있게 사용함.
           >
           > 한글을 사용하며 약어 사용은 지양함
           >
           > 애매한 용어 사용 자제
           >
           > 새로운 지표를 산출할 경우 풀어쓰는게 원칙이며 필요시 연산식을 반드시 명시함

      2. 스타일가이드(데이터 시각화)

         - 청자의 인지를 돕기 위해 도표 및 그래프에 일관된 색상을 활용함

           > 일관된 색상만 사용하여 타 유저들이 한번에 그래프를 파악할 수 있도록 함

5. QNA

   - 내부가이드 만들 때 참조한 레퍼런스가 있는지?
     - 레퍼런스는 없었고 직접 작업을 하면서 필요할 것 같다는 지침을 정리한 것
   - 코드의 건강함을 포기한다?
     - 장기적으로 보았을 때에는 당연히 우려되는 점이 있음. 매 작업에 대해 코드리뷰를 하지는 않지만 코드리뷰 한번 할 때마다 정확하게 개선안을 제안하도록 함. -> 그러나 바로 반영하지는 못함... 그래도 리팩토링할 수 있도록 제시는 한다는 것
   - 

####7. 온라인 뉴스 댓글 생태계를 흐리는 어뷰저 분석기

---

![img](https://datayanolja.github.io/images/presenters/moonjihyung.png)발표자 | 문지형

사람을 위한 데이터 분석과 모델링을 하기 위해 노력하는 머신러닝 엔지니어입니다. 사이드 프로젝트가 종종 삶의 원동력이 되기도 합니다. 네이버 파파고 팀 머신러닝 엔지니어

1. 개요
   - 사이버 여론조작에 대한 경각심; 드루킹 사건 등
2. 진행
   1. 데이터 수집
      - 2006.04.26 ~ 2018.05.25까지의 네이버뉴스를 크롤링
      - 6개분야별 가장 많이 본 뉴스 30건을 대상으로 하여 기사 제목, 내용, 댓글과 아이디를 수집함
   2. 데이터 탐색
      - 많이 본 뉴스가 전체를 대표할 수 있을까? 월별 뉴스 댓글 수를 분석해봄
        - 시기별 나무위키의 네이버뉴스 설명을 그래프의 수치와 대조함
      - 2016년 이후에 정치적인 이슈로 댓글 수가 폭증함을 알 수 있었음
   3. 어뷰저 탐색
      - 어뷰저는 무엇일까? -> 이를 정의한 바대로 분석의 방향이 잡힘
      - 어뷰징의 목적과 문제가 되는 상황에 대해 사전정의
        - 어뷰징의 목적: 베스트 댓글이 되어서 상단에 고정됨을 목표로 함
        - 문제가 되는 상황: 여론을 대표할 수 없는 댓글이 상단에 고정되어서 잘못된 여론 형성이 되는 점
      - 어뷰징의 목적을 달성하였고, 특정세력이 배후에 있지 않고는 할 수 없는 특정액션을 하는 아이디를 어뷰저라고 지칭
   4. 어뷰저 criteria 1: top 10
      - 자주 순위권 내에 드는 유저를 top유저라고 정의
      - top user 그룹은 어떻게 분류할 것인가?
        - 골수유저; 생태계를 잘 알고 있기 때문에 내용과 연관 높은 글을 잘 작성할 것
        - 어뷰저; 전략과 무관하게 보이지 않는 세력에 의해 높은공감을 받았을 것
      - 베댓이 되었던 수를 확률변수 X로 정의
   5. 어뷰저 criteria 2: abnormal
      - 어뷰저는 top10 성공률이 타 유저보다 월등히 높을 것이라는 가정
      - Top user의 top 10 성공률을 확률 변수 X라고 했을 때 histogram과 GMM(n=1)로 추정하여 확률분포로 나타냈을 때
        - 굉장히 특이한 데이터가 있음 (90퍼센으 이상의 적중률을 보이는 유저)
        - abnormal한 패턴이라고 생각됨
   6. 어뷰저 의심 유저
      - X가 90퍼센트 이상인 유저들의 실제 댓글 내용을 읽어보면 단순 복붙인 경우가 많았음
      - 다른 분야임에도 불구하고 같은 내용을 복붙한 경우도 있음
3. 어뷰저를 발견한 이후 어떤 액션을 취해야할까?
   1. 욕망을 바꿀 수 없으니, rule을 바꿔보자 -> reddit에서 사용하는 정렬 방식을 차용
      - 네이버 뉴스 정렬 방식을 살펴보면 순공감순, 공감비율순을 조정할 필요가 있음
      - wilson score를 도출 -> 공감비율에 베니핏이 많이 가는 현상을 방지
      - Contriversial; 첨예하게 대립되는 댓글 순으로 정렬할 수 있음
      - 비공감순정렬? wilson score를 사용해서 비공감순 정렬 시도
4. 향후
   - 텍스트 기반의 metric을 활용해서 기사내용과 연관이 높은 댓글 순으로 정렬하는 방법도 고안하고 싶음
   - https://inmoonlight.github.io
5. QNA
   - 크롤링은 어떻게 했는지? 막히는 점 없었는지?
     - 깃허브에 크롤러를 올려두었음
   - 가중치를 어떻게 설정하였는지?
     - 데이터를 본 범주 내에서는 그 가중치가 최선이었음... 

